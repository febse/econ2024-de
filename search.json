[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Einführung in die Ökonometrie",
    "section": "",
    "text": "Setup\nIn den Übungen werden wir die Programmiersprache Python verwenden, um die in der Vorlesung behandelten Themen zu erklären und an realen Daten zu üben. Die Übungen werden in Form von Jupyter Notebooks bereitgestellt, die Sie in Ihrer eigenen Umgebung ausführen können.\nDeswegen ist es notwendig, zuerst eine Arbeitsumgebung einzurichten.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "index.html#git",
    "href": "index.html#git",
    "title": "Einführung in die Ökonometrie",
    "section": "Git",
    "text": "Git\nDie Übungen werden in einem Git-Repository auf GitHub bereitgestellt. Git ist ein Versionskontrollsystem, das die Änderungen an Dateien verfolgt und es ermöglicht, verschiedene Versionen von Dateien zu speichern. GitHub ist eine Plattform, die auf Git basiert und es ermöglicht, Git-Repositories zu hosten und zu teilen.\nWir werden Git verwenden, um die Übungen herunterzuladen und um Ihre Lösungen hochzuladen.\nInstallieren Sie Git von https://git-scm.com/downloads. Wählen Sie die Version, die zu Ihrem Betriebssystem passt und folgen Sie den Anweisungen. Lassen Sie die Standardeinstellungen unverändert, es sei denn, Sie wissen ganz genau, was Sie tun.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "index.html#github-account",
    "href": "index.html#github-account",
    "title": "Einführung in die Ökonometrie",
    "section": "GitHub Account",
    "text": "GitHub Account\nFür die Übungen brauchen Sie ein GitHub.com Konto:\n\nErstellen Sie ein Konto auf https://github.com/signup.\nBei der Registrierung wählen Sie ein Passwort, das Sie nirgendwo anders verwenden. Am Ende des Semesters können Sie Ihr Passwort ändern.\nEmpfehlung: Aktivieren Sie die Zwei-Faktor-Authentifizierung (2FA) in den Einstellungen Ihres GitHub-Kontos.\nEmpfehlung: Beantragen Sie die Studentenvorteile, um kostenlosen Zugang zu GitHub Copilot zu erhalten, einem KI-gestützten Code-Completion- und Chat-Tool.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "index.html#visual-studio-code",
    "href": "index.html#visual-studio-code",
    "title": "Einführung in die Ökonometrie",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\nVisual Studio Code (VSC) ist ein Open-Source-Code-Editor, den wir für die Arbeit mit Python benutzen werden.\nInstallieren Sie Visual Studio Code von https://code.visualstudio.com/. Wählen Sie die Version, die zu Ihrem Betriebssystem passt und folgen Sie den Anweisungen.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "index.html#kursrepository-klonen",
    "href": "index.html#kursrepository-klonen",
    "title": "Einführung in die Ökonometrie",
    "section": "Kursrepository klonen",
    "text": "Kursrepository klonen\nÖffnen Sie VSC und drücken Sie Ctrl+Shift+P. Suchen Sie nach “Git: Clone” und drücken Sie Enter. Geben Sie die URL des Kursrepositories ein:\nhttp://github.com/febse/econ2024-de.git\nWählen Sie einen Ordner auf Ihrem Computer, in dem Sie das Repository speichern möchten. VSC wird das Repository herunterladen und fragen, ob Sie es öffnen möchten. Klicken Sie auf “Open”.\nWarten Sie einen Moment, bis VSC das Repository geöffnet hat. Sie sollten eine Ordnerstruktur sehen, die den Inhalt des Repositories darstellt. VSC wird Sie fragen, ob Sie die empfohlenen Erweiterungen installieren möchten. Klicken Sie auf “Install All”.\nAm Ende sollten Sie die folgenden Erweiterungen installiert haben:\n\nms-python.autopep8\nms-python.python\nms-python.debugpy\nms-vscode-remote.remote-containers\nms-toolsai.jupyter\ngithub.codespaces\ngithub.copilot\n\nDrucken Sie Ctrl+Shift+X. Dieses wird die Erweiterungsansicht öffnen. Überprüfen Sie ob alle Erweiterungen installiert sind. Falls nicht, kopieren Sie die Kennzeichen (z.B. ms-python.autopep8) der fehlenden Erweiterungen von der Liste (oben), finden Sie die Erweiterungen in der Erweiterungsansicht und installieren Sie sie manuell.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "Einführung in die Ökonometrie",
    "section": "Python",
    "text": "Python\nPython ist zurzeit eine der meistbenutzten Programmiersprachen in der Welt. Es ist einfach zu lernen und hat eine große Community, die viele Bibliotheken und Frameworks entwickelt hat. In der Welt der Datenwissenschaft werden meistens Python und R benutzt. In den letzten Jahren stieg die Popularität von Python.\n\nCodespaces (empfohlen)\nUm die Konfiguration zu vereinfachen, können Sie Codespaces benutzen. In den ersten Übungen werden wir eine Anleitung zeigen, wie Sie Codespaces benutzen können.\n\n\nMiniconda\nEs gibt viele Möglichkeiten, wie Sie Python local auf Ihrem Rechner installieren können. Hier zeigen wir die Installation mit Miniconda. Conda ist ein Paketmanager, der es ermöglicht, Python-Umgebungen zu verwalten. Eine Python-Umgebung ist eine isolierte Instanz von Python, die es ermöglicht, verschiedene Versionen von Python und verschiedenen Bibliotheken zu verwenden, ohne dass sie sich gegenseitig beeinflussen. Das ist vor allem dann nützlich, wenn Sie an verschiedenen Projekten arbeiten, die unterschiedliche Anforderungen an Python und Bibliotheken haben.\n\nLaden Sie das Miniconda Installationspaket von https://docs.conda.io/en/latest/miniconda.html für Ihr Betriebssystem.\nStarten Sie das Packet und folgen Sie den Anweisungen. Lassen Sie die Standardeinstellungen unverändert, es sei denn, Sie wissen was Sie tun.\nInstallieren Sie Conda in einem Ordner ohne Leerzeichen und nur mit ASCII Zeichen im Pfad (d.h. keine Kyrillischen Zeichen, keine Umlaute, etc.), z.B. C:\\Miniconda3 oder C:\\Users\\boyko\\Miniconda3.\nNachdem Miniconda installiert ist, öffnen Sie VSC. Drücken Sie Ctrl+Shift+P und starten Sie zu schreiben: “Python: Create environment”. Wenn die Option in dem Dropdown-Menü erscheint, wählen Sie sie und drucken Sie Enter.\nWählen Sie “Conda”.\nWählen Sie die Python-3.11 Version.\nDieses wird einen Order namens .conda erstellen.\nFalls Sie diese Schritte in dem Order des Kursrepositories gemacht haben, werden dann automatisch die notwendingen Bibliotheken installiert.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "index.html#pycharm-optional",
    "href": "index.html#pycharm-optional",
    "title": "Einführung in die Ökonometrie",
    "section": "Pycharm (optional)",
    "text": "Pycharm (optional)\nStatt VSC können Sie auch PyCharm oder DataSpell benutzen. PyCharm ist eine integrierte Entwicklungsumgebung (IDE) für Python, die von JetBrains entwickelt wurde. PyCharm ist eine der beliebtesten Python-IDEs und bietet viele Funktionen, die die Entwicklung von Python-Programmen erleichtern. Falls Sie mit PyCharm oder DataSpell experimentieren möchten, bieten wir für die Übungen kostenfreie Lizenzen an.\n\nRegistrieren Sie sich auf https://account.jetbrains.com/signup mit Ihrer Universitäts-E-Mail-Adresse (muss auf feb.uni-sofia.bg oder feba.uni-sofia.bg enden).\nLaden Sie Pycharm von https://www.jetbrains.com/pycharm/download/ herunter und installieren Sie es.\nBeim Aktivierungsdialog wählen Sie “Licence Server” und geben Sie https://febs.fls.jetbrains.com/ ein.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "01-Python/01-01-Basics.html",
    "href": "01-Python/01-01-Basics.html",
    "title": "1  Python Basics",
    "section": "",
    "text": "1.1 Jupyter Notebooks in VS Code\nHäufig verwendete Tastenkombinationen\nShortcuts für den Kommando-Modus (Edit Mode)\nShortcuts für den Edit Modus - Alt + Shift + ↑: Die aktuelle Zeile nach oben kopieren (Fokus bleibt auf der aktuellen Zeile) - Alt + Shift + ↓: Die aktuelle Zeile nach unten kopieren (Fokus verlagert sich zur nächsten Zeile) - Ctrl+Enter/Shift+Enter: Markierte Zelle ausführen und Fokus auf die nächste Zelle setzen\nShortcuts für beide Modi",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "01-Python/01-01-Basics.html#jupyter-notebooks-in-vs-code",
    "href": "01-Python/01-01-Basics.html#jupyter-notebooks-in-vs-code",
    "title": "1  Python Basics",
    "section": "",
    "text": "Enter: Bearbeitungsmodus einschalten\nEsc: Kommando-Modus einschalten\n\n\n\nA : Eine Zelle oberhalb der aktuellen Zelle einfügen\nB : Eine Zelle unterhalb der aktuellen Zelle einfügen\nAlt + ↑: Zelle nach oben verschieben\nAlt + ↓: Zelle nach unten verschieben\nAlt + Shift + ↑: Zelle nach oben kopieren (Fokus bleibt auf der aktuellen Zelle)\nAlt + Shift + ↓: Zelle nach unten kopieren (der Fokus wechselt zur nächsten Zelle)\ndd : Zelle löschen\nz : Die letzte Änderung rückgängig machen\nM : Die Zelle in eine Markdown-Zelle umwandeln\nY: Die Zelle in eine Code-Zelle umwandeln\n\n\n\n\nCtrl+Enter : Die aktuelle Zelle ausführen (Fokus bleibt auf der aktuellen Zelle)\nShift+Enter : Die aktuelle Zelle ausführen und Fokus auf die nächste Zelle setzen\nAlt+Enter : Die aktuelle Zelle ausführen und eine neue Zelle einfügen",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "01-Python/01-01-Basics.html#arithmetische-operationen",
    "href": "01-Python/01-01-Basics.html#arithmetische-operationen",
    "title": "1  Python Basics",
    "section": "1.2 Arithmetische Operationen",
    "text": "1.2 Arithmetische Operationen\n\n# Addition\n1 + 1\n\n2\n\n\nDie Varianz von Y ist größer als die Varianz von X. Das bedeutet, daß die Werte, die Y produziert, weiter von ihrem Erwartungswert entfernt sein werden als die Werte, die X produziert.\n\n# Division\n1 / 2\n\n0.5\n\n\n\n# Multiplikation\n2 * 3\n\n6\n\n\n\n# Potenz\n2.3 ** 8\n\n783.1098528099996\n\n\n\n# Typ\ntype(1)\n\nint\n\n\n\ntype(1.0)\n\nfloat\n\n\n\n1.2.1 Zuweisung (Assignment)\nWir können Werte Variablen zuweisen. In Python verwenden wir dazu das Gleichheitszeichen =. Der Wert auf der rechten Seite des Gleichheitszeichens wird der Variablen auf der linken Seite zugewiesen. Anders als in der Mathematik, wo die Gleichung x = 3 bedeutet, dass x gleich 3 ist, bedeutet die Zuweisung x = 3 in Python, dass der Wert 3 in der Variablen x gespeichert wird. Python speichert dann den Wert 3 im Speicher des Rechners und weist ihm den Namen x zu.\n\nx = 1\ny = 5\n\n\nx + y\n\n6\n\n\n\ntype(x)\n\nint\n\n\n\n\n1.2.2 Zeichenketten (Strings)\nAußer Zahlen können wir auch Zeichenketten (Strings) speichern.\n\nz = \"Hallo, Welt!\"\nz\n\n'Hallo, Welt!'\n\n\n\nprint(z)\n\nHallo, Welt!\n\n\nZeichenketten können wir mit + verketten.\n\nz1 = z + \" Ich bin da!\"\nprint(z1)\n\nHallo, Welt! Ich bin da!\n\n\nZeichenketten können wir mit * “vervielfachen”.\n\nprint(z * 3)\n\nHallo, Welt!Hallo, Welt!Hallo, Welt!\n\n\nZeichenketten können wir mit len die Länge (Anzahl der Zeichen) bestimmen.\n\nlen(z)\n\n12\n\n\nZeichenketten können wir mit split in eine Liste von Wörtern aufteilen.\n\nz.split(\",\")\n\n['Hallo', ' Welt!']\n\n\n\nz.split(\" \")\n\n['Hallo,', 'Welt!']\n\n\nWir können Teile einer Zeichenkette mit eckigen Klammern auswählen. Dabei beginnt der Index bei 0.\n\nz[0]\n\n'H'\n\n\n\nz[1]\n\n'a'\n\n\n\nz[0:5]\n\n'Hallo'\n\n\n\nfor char in z:\n    print(char)\n\nH\na\nl\nl\no\n,\n \nW\ne\nl\nt\n!\n\n\n\n\n1.2.3 Logische Werte (Booleans)\nIn Python gibt es auch logische Werte (Booleans). Diese können nur zwei Werte annehmen: True und False. In den meisten Fällen werden wir diese Werte verwenden, um Bedingungen zu formulieren.\n\n1 &gt; 2\n\nFalse\n\n\n\n1 &lt; 3\n\nTrue\n\n\n\n2 == 2\n\nTrue\n\n\n\n\"hallo\" == \"hallo\"\n\nTrue\n\n\n\n\"hallo\" == \"Hallo\"\n\nFalse",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "01-Python/01-01-Basics.html#listen-und-tupel",
    "href": "01-Python/01-01-Basics.html#listen-und-tupel",
    "title": "1  Python Basics",
    "section": "1.3 Listen und Tupel",
    "text": "1.3 Listen und Tupel\nIn vielen Fällen werden wir nicht nur einzelne Werte, sondern mehrere Werte zusammen verwenden wollen. Als Beispiel nehmen wir eine Liste mit monatlichem Stromverbrauch eines Unternehmens (für 12 Monate in EUR).\n\nstromverbrauch = [100, 120, 80, 90, 110, 100, 120, 80, 90.12, 110, 100, 120.22]\n\nWir können auf einzelne Elemente einer Liste zugreifen, indem wir den Index des Elements in eckigen Klammern hinter den Namen der Liste schreiben. Wie in den meisten Programmiersprachen beginnt der Index bei 0.\n\nstromverbrauch[0]\n\n100\n\n\n\nstromverbrauch[1]\n\n120\n\n\n\nstromverbrauch[11]\n\n120.22\n\n\nVersuchen wir auf ein Element zuzugreifen, das nicht existiert, erhalten wir einen Fehler.\n\n# stromverbrauch[12]\n\nDie Länge einer Liste können wir mit der Funktion len bestimmen. Diese gibt uns die Anzahl der Elemente in der Liste zurück.\n\nlen(stromverbrauch)\n\n12\n\n\nWir können auch auf die letzten Elemente einer Liste zugreifen, indem wir negative Indizes verwenden.\n\nstromverbrauch[len(stromverbrauch) - 1]\n\n120.22\n\n\n\nstromverbrauch[-1]\n\n120.22\n\n\nWir können auch auf mehrere Elemente einer Liste zugreifen, indem wir den Index des ersten Elements und den Index des letzten Elements in eckigen Klammern hinter den Namen der Liste schreiben. Der Index des letzten Elements ist dabei exklusiv.\n\nstromverbrauch[0:3]\n\n[100, 120, 80]\n\n\nDas ist das gleiche wie\n\nstromverbrauch[:3]\n\n[100, 120, 80]\n\n\n\nstromverbrauch[0:3] == stromverbrauch[:3]\n\nTrue\n\n\n\nstromverbrauch[3:6]\n\n[90, 110, 100]\n\n\nWir können einzelne Elemente einer Liste auch verändern.\n\nstromverbrauch[0] = 1\nstromverbrauch\n\n[1, 120, 80, 90, 110, 100, 120, 80, 90.12, 110, 100, 120.22]\n\n\nWir können auch mehrere Elemente einer Liste auf einmal verändern.\n\nstromverbrauch[0:3] = [5, 5, 8]\nstromverbrauch\n\n[5, 5, 8, 90, 110, 100, 120, 80, 90.12, 110, 100, 120.22]\n\n\nTuple sind wie Listen, nur dass sie nicht verändert werden können. Wir können sie mit runden Klammern erstellen. Im folgenden Beispiel erstellen wir ein Tuple mit den Monatsnamen. Dann versuchen wir, das erste Element zu verändern. Das funktioniert nicht.\n\nmonatsnamen = (\"Januar\", \"Februar\", \"März\", \"April\", \"Mai\", \"Juni\", \"Juli\",\n               \"August\", \"September\", \"Oktober\", \"November\", \"Dezember\")\n\n\n# monatsnamen[0] = \"Jänner\"\n\n\n1.3.1 Schleifen\nIn vielen Fällen wollen wir eine Operation auf alle Elemente einer Liste anwenden. Dafür können wir Schleifen verwenden.\n\nfor g in stromverbrauch:\n    print(f\"Der Stromverbrauch war {g} EUR\")\n\nDer Stromverbrauch war 5 EUR\nDer Stromverbrauch war 5 EUR\nDer Stromverbrauch war 8 EUR\nDer Stromverbrauch war 90 EUR\nDer Stromverbrauch war 110 EUR\nDer Stromverbrauch war 100 EUR\nDer Stromverbrauch war 120 EUR\nDer Stromverbrauch war 80 EUR\nDer Stromverbrauch war 90.12 EUR\nDer Stromverbrauch war 110 EUR\nDer Stromverbrauch war 100 EUR\nDer Stromverbrauch war 120.22 EUR\n\n\n\nfor idx, g in enumerate(stromverbrauch):\n    print(f\"Der Stromverbrauch im Monat {monatsnamen[idx]} war {g} EUR.\")\n\nDer Stromverbrauch im Monat Januar war 5 EUR.\nDer Stromverbrauch im Monat Februar war 5 EUR.\nDer Stromverbrauch im Monat März war 8 EUR.\nDer Stromverbrauch im Monat April war 90 EUR.\nDer Stromverbrauch im Monat Mai war 110 EUR.\nDer Stromverbrauch im Monat Juni war 100 EUR.\nDer Stromverbrauch im Monat Juli war 120 EUR.\nDer Stromverbrauch im Monat August war 80 EUR.\nDer Stromverbrauch im Monat September war 90.12 EUR.\nDer Stromverbrauch im Monat Oktober war 110 EUR.\nDer Stromverbrauch im Monat November war 100 EUR.\nDer Stromverbrauch im Monat Dezember war 120.22 EUR.\n\n\nDie Namen der Variablen in der for-Schleife können wir frei wählen (diese müssen allerdings valide Variablennamen sein).\n\nfor i, verbrauch in enumerate(stromverbrauch):\n    print(f\"Der Stromverbrauch im Monat {monatsnamen[i]} war {verbrauch} EUR.\")\n\nDer Stromverbrauch im Monat Januar war 5 EUR.\nDer Stromverbrauch im Monat Februar war 5 EUR.\nDer Stromverbrauch im Monat März war 8 EUR.\nDer Stromverbrauch im Monat April war 90 EUR.\nDer Stromverbrauch im Monat Mai war 110 EUR.\nDer Stromverbrauch im Monat Juni war 100 EUR.\nDer Stromverbrauch im Monat Juli war 120 EUR.\nDer Stromverbrauch im Monat August war 80 EUR.\nDer Stromverbrauch im Monat September war 90.12 EUR.\nDer Stromverbrauch im Monat Oktober war 110 EUR.\nDer Stromverbrauch im Monat November war 100 EUR.\nDer Stromverbrauch im Monat Dezember war 120.22 EUR.",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "01-Python/01-01-Basics.html#dictionaries",
    "href": "01-Python/01-01-Basics.html#dictionaries",
    "title": "1  Python Basics",
    "section": "3.1 Dictionaries",
    "text": "3.1 Dictionaries\nEs is sehr häufig, dass wir nicht nur einzelne Werte, sondern ganze Datensätze verwenden wollen. Dafür können wir Dictionaries verwenden. Dictionaries sind wie Listen, nur dass wir nicht mit Indizes, sondern mit Schlüsseln auf die Elemente zugreifen. Schlüssel können beliebige unveränderbare Datentypen sein (z.B. Strings, Zahlen, Tupel). Werte können beliebige Datentypen sein. Wir können Dictionaries mit geschweiften Klammern erstellen. Die Werte in einem Dictionary werden mit einem Doppelpunkt vom Schlüssel getrennt. Die einzelnen Elemente werden mit Kommas getrennt.\nDie Ordnung der Elemente ist nicht definiert.\n\nstromverbrauch_dict = {\n    \"Januar\": 100,\n    \"Februar\": 120,\n    \"März\": 80,\n    \"April\": 90,\n    \"Mai\": 110,\n    \"Juni\": 100,\n    \"Juli\": 120,\n    \"August\": 80,\n    \"September\": 90.12,\n    \"Oktober\": 110,\n    \"November\": 100,\n    \"Dezember\": 120.22\n}\n\n\nstromverbrauch_dict[\"Januar\"]\n\n100\n\n\n\n# stromverbrauch_dict[\"FDFFF\"]\n\nWir können auf\n\nstromverbrauch_dict.get(\"Januar\")\n\n100\n\n\n\n# stromverbrauch_dict.get(\"FDFF\") == None\n\nWir können über die Schlüssel eines Dictionaries iterieren.\n\nfor key in stromverbrauch_dict:\n    print(key)\n\nJanuar\nFebruar\nMärz\nApril\nMai\nJuni\nJuli\nAugust\nSeptember\nOktober\nNovember\nDezember\n\n\nWir können auch über die Schlüssel und Werte eines Dictionaries iterieren.\n\nfor key, value in stromverbrauch_dict.items():\n    print(f\"Im {key} war der Stromverbrauch {value} EUR.\")\n\nIm Januar war der Stromverbrauch 100 EUR.\nIm Februar war der Stromverbrauch 120 EUR.\nIm März war der Stromverbrauch 80 EUR.\nIm April war der Stromverbrauch 90 EUR.\nIm Mai war der Stromverbrauch 110 EUR.\nIm Juni war der Stromverbrauch 100 EUR.\nIm Juli war der Stromverbrauch 120 EUR.\nIm August war der Stromverbrauch 80 EUR.\nIm September war der Stromverbrauch 90.12 EUR.\nIm Oktober war der Stromverbrauch 110 EUR.\nIm November war der Stromverbrauch 100 EUR.\nIm Dezember war der Stromverbrauch 120.22 EUR.",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "01-Python/01-01-Basics.html#klassen-und-objekte",
    "href": "01-Python/01-01-Basics.html#klassen-und-objekte",
    "title": "1  Python Basics",
    "section": "4.1 Klassen und Objekte",
    "text": "4.1 Klassen und Objekte\nBisher haben wir verschiedene Datentypen gesehen, drei verschiedene Datenstrukturen (Listen, Tupel und Dictionaries) und Funktionen gesehen. In Python gibt es auch Objekte, die Daten und Funktionen zusammenbinden. Funktionen, die zu einem Objekt gehören, nennen wir Methoden. Objekte werden aus Klassen erstellt. Klassen sind wie Baupläne für Objekte. In den Übungen werden wir keine Klassen selber definieren, allerdings werden wir Objekte wie DataFrame, Series, etc. verwenden, die aus Klassen erstellt wurden. Deswegen ist es wichtig, dass Sie wissen, wie Sie mit Objekten umgehen können.\nZuerst definieren wir eine Klasse Person. Diese hat zwei Attribute: name und age. Die Klasse erlaubt uns, ein Objekt zu erstellen, das diese beiden Attribute hat. Wir können auch eine Methode say_hello definieren, die uns erlaubt, “Hallo” und den Namen der Person auszugeben. Alle Methoden, wie z.B. say_hello erhalten das Argument self, das das Objekt selbst ist. Dadurch können wir über Methoden auf die Eigenschaften des Objekts zugreifen.\n\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def say_hello(self):\n        print(\"Hi, my name is\", self.name, \"and I am\", self.age, \"years old.\")\n\n\nivan = Person(\"Ivan\", 20)\nbetty = Person(\"Betty\", 25)\n\nprint(ivan.name)\nprint(ivan.age)\n\nIvan\n20\n\n\n\nivan.say_hello()\n\nHi, my name is Ivan and I am 20 years old.\n\n\n\nbetty.say_hello()\n\nHi, my name is Betty and I am 25 years old.\n\n\n\n4.1.1 Lambdafunktionen\nManchmal möchten wir eine Funktion nur einmal verwenden. Dafür können wir Lambda Funktionen verwenden. Diese Funktionen haben keinen Namen. Wir können sie mit dem Schlüsselwort lambda erstellen. Die Argumente der Funktion werden durch ein Komma getrennt hinter dem Schlüsselwort lambda geschrieben. Der Rückgabewert der Funktion wird nach einem Doppelpunkt geschrieben.\nZum Beispiel können wir eine Lambdafunktion verwenden, um den Stromverbrauch in BGN zu berechnen.\n\nstromverbrauch_bgn = map(lambda x: x * 1.955, stromverbrauch)",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "01-Python/01-02-Basics-Ex.html",
    "href": "01-Python/01-02-Basics-Ex.html",
    "title": "2  Übung: Python Basics",
    "section": "",
    "text": "2.0.1 Aufgabe 1\n\nBerechnen Sie die Summe der Zahlen 32.8 und 45.1\nBerechnen Sie die Differenz der Zahlen 32.8 und 45.1\nSpeichern Sie die Zahl 32.8 in a und die Zahl 45.1 in b. Berechnen Sie das Verhältnis von a zu b und speichern Sie das Ergebnis in c.\n\n\n\n2.0.2 Aufgabe 2\nErstellen Sie eine Liste mit den Zahlen 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 und speichern Sie diese in der Variable numbers. - Geben Sie die Liste aus (mit print()) - Geben Sie das erste Element der Liste aus - Geben Sie das letzte Element der Liste aus - Geben Sie das dritte Element der Liste aus - Geben Sie das dritte und das siebte Element der Liste aus - Ersetzen Sie das vierte Element der Liste durch die Zahl 42\n\n\n2.0.3 Aufgabe 3\nErstellen Sie zwei Listen names und ages mit den folgenden Elementen: - names: “Alice”, “Bob”, “Charlie”, “Dave”, “Eve” - ages: 17, 42, 30, 37, 21\n\nGeben Sie die Liste names aus\nSchreiben Sie eine for-Schleife, die die Namen und das Alter der Personen ausdruckt. Die Ausgabe soll folgendermaßen aussehen: “Alice ist 23 Jahre alt”\nSie möchten nur die Namen der Personen ausdrucken, die älter als 30 Jahre sind. Schreiben Sie eine for-Schleife, die das tut.\nSie möchten eine neue Liste erstellen, die nur die Namen der Personen enthält, die älter als 30 Jahre sind. Schreiben Sie eine Listenabstraktion, die das tut.\n\n\n\n2.0.4 Aufgabe 4\nIn dieser Aufgabe werden wir eine Funktion is_in_list schreiben, die einen Namen als Argument annimmt und True oder False zurückgibt, falls der Name einer Person entspricht, die in der Liste names aus Aufgabe 3 enthalten ist.\n\n\n2.0.5 Aufgabe 5\nIn dieser Aufgabe werden wir eine Funktion get_age schreiben, die einen Namen als Argument annimmt und das Alter der Person zurückgibt, falls der Name einer Person entspricht, die in der Liste names aus Aufgabe 3 enthalten ist. Falls der Name nicht in der Liste enthalten ist, soll die Funktion None zurückgeben.\n\n\n2.0.6 Aufgabe 6\nIn dieser Aufgabe werden wir eine Funktion is_eligible schreiben, die True, False oder None zurückgibt, je nachdem ob eine Person wahlberechtigt ist oder nicht. Die Funktion soll einen Namen als Argument annehmen und das Alter der Person zurückgeben, falls der Name einer Person entspricht, die in der Liste names aus Aufgabe 3 enthalten ist. Falls der Name nicht in der Liste enthalten ist, soll die Funktion None zurückgeben. Falls der Name in der Liste enthalten ist, soll die Funktion True zurückgeben, falls die Person älter als 18 Jahre ist, andernfalls soll die Funktion False zurückgeben.\n\n\n2.0.7 Aufgabe 7\nIn dieser Aufgabe werden wir ein Dictionary erstellen, das die Namen aus Aufgabe 3 als Schlüssel und die Alter aus Aufgabe 3 als Werte enthält. Speichern Sie das Dictionary in der Variable persons.\n\nBenutzen Sie das Dictionary, um das Alter von Bob auszudrucken.\nÄndern Sie das Alter von Alice auf 43 Jahre.\nFügen Sie eine neue Person “Frank” mit dem Alter 33 Jahre hinzu.\nSchreiben Sie eine for-Schleife, die die Namen und das Alter der Personen ausdruckt. Die Ausgabe soll folgendermaßen aussehen: “Alice ist 23 Jahre alt”\nSie möchten nur die Namen der Personen ausdrucken, die mit “A” anfangen. Schreiben Sie eine for-Schleife, die über persons iteriert und das tut.",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Übung: Python Basics</span>"
    ]
  },
  {
    "objectID": "01-Python/01-03-Numpy.html",
    "href": "01-Python/01-03-Numpy.html",
    "title": "3  NumPy",
    "section": "",
    "text": "In der vorherigen Lektion haben wir uns mit Listen beschäftigt. Diese sind sehr flexibel, aber auch sehr langsam. Mit kleinen Listen werden wir das nicht merken, aber wenn wir mit großen Datenmengen arbeiten, dann können lange Laufzeiten zum Problem werden.\nNumPy (Numerical Python) ist ein Paket, das viele Funktionen enthält, die für die Arbeit mit großen Datenmengen geeignet sind.\n\nimport numpy as np\nfrom time import process_time\n\n\n# Wir können eine Liste in ein NumPy-Array umwandeln\npython_list = range(int(1e6))\nnumpy_array = np.array(python_list)\n\n\nstart_zeit = process_time()\n\na_list_plus_2 = [i + 2 for i in python_list]\n\nend_zeit = process_time()\nround(end_zeit - start_zeit, 5)\n\n0.034\n\n\n\nstart_zeit_1 = process_time()\n\na_array_plus_2 = numpy_array + 2\n\nend_zeit_1 = process_time()\nround(end_zeit_1 - start_zeit_1, 5)\n\n0.00258\n\n\n\nx = np.array([2, 7, 5, 2])\ny = np.ones(4)\ny\n\narray([1., 1., 1., 1.])\n\n\nNumPy-Arrays sind sehr ähnlich zu Listen, aber sie haben einige zusätzliche Eigenschaften. Zum Beispiel können wir mit NumPy-Arrays rechnen. Wenn wir zwei NumPy-Arrays addieren, dann werden die Elemente an der gleichen Stelle addiert. Diese Syntax ist sehr intuitiv und einfach zu lesen.\n\nz1 = x + 1 \nz2 = x + y\nz1\n\narray([3, 8, 6, 3])\n\n\n\nz2\n\narray([3., 8., 6., 3.])\n\n\nDasselbe funktioniert mit Python-Listen nicht. Wenn wir zwei Listen addieren, dann werden die Elemente an der gleichen Stelle nicht addiert, sondern die Listen werden aneinander gehängt.\n\nx_list = [2, 7, 5, 2]\ny_list = [1, 1, 1, 1]\nx_plus_y_list = x_list + y_list\nx_plus_y_list\n\n[2, 7, 5, 2, 1, 1, 1, 1]\n\n\n\n# x_list + 1\n\nNumPy-Arrays haben auch einige Methoden, die wir mit Listen nicht haben. Zum Beispiel können wir die Summe aller Elemente eines Arrays berechnen.\n\n# Summe der Elemente\nx.sum()\n\n16\n\n\n\n# Durchschnitt der Elemente (Arithmetischer Mittelwert)\n\nx.mean()\n\n4.0\n\n\n\n# Standardabweichung\nx.std()\n\n2.1213203435596424\n\n\n\n# Minimum\nx.min()\n\n2\n\n\n\n# Maximum\nx.max()\n\n7\n\n\n\n# Der Index des Minimums\nx.argmin()\n\n0\n\n\n\n# Der Index des Maximums\nx.argmax()\n\n1\n\n\n\n3.0.1 Slicing\nWir können auch auf die Elemente eines Arrays zugreifen, indem wir einen Index angeben. Der Index beginnt bei 0.\n\nz = np.array([\"a\", \"b\", \"c\", \"d\", \"e\"])\n\n\n# Das erste Element\nz[0]\n\n'a'\n\n\n\n# Alle Elemente bis zur Indexposition 2 (exklusive)\n# Achten Sie darauf, dass der Index 0-basiert ist und daher das dritte Element ist eine Indexposition 2 hat\nz[:2]\n\narray(['a', 'b'], dtype='&lt;U1')\n\n\n\nz[-3]\n\n'c'\n\n\n\nz[-3:]\n\narray(['c', 'd', 'e'], dtype='&lt;U1')\n\n\n\nz[1:3]\n\narray(['b', 'c'], dtype='&lt;U1')\n\n\n\nz[::2]\n\narray(['a', 'c', 'e'], dtype='&lt;U1')\n\n\n\nz[::-1]\n\narray(['e', 'd', 'c', 'b', 'a'], dtype='&lt;U1')\n\n\n\n\n3.0.2 Arrays und Listen Zuweisung\nEine zugängige Erläuterung, wie Werte im Speicher des Rechners gespeichert werden, finden Sie in hier.\n\nz[1] = \"x\"\nz\n\narray(['a', 'x', 'c', 'd', 'e'], dtype='&lt;U1')\n\n\n\nz1 = z\n\n\nz[0] = \"X\"\n\n\nz1\n\narray(['X', 'x', 'c', 'd', 'e'], dtype='&lt;U1')\n\n\n\nz1 == z\n\narray([ True,  True,  True,  True,  True])\n\n\n\nprint(id(z1))\nprint(id(z))\n\n139876194347184\n139876194347184\n\n\n\nl1 = [1, 2, 3]\nl2 = l1\n\nprint(id(l1))\nprint(id(l2))\n\n139877223683584\n139877223683584\n\n\n\ns1 = \"Some string\"\ns2 = s1\n\nprint(id(s1))\nprint(id(s2))\n\n# s1[0] = \"X\"\n\n139877223682352\n139877223682352",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "01-Python/01-04-Pandas.html",
    "href": "01-Python/01-04-Pandas.html",
    "title": "4  Pandas",
    "section": "",
    "text": "4.1 Series\nDie einfachste Datenstruktur in Pandas ist die Series. Eine Series ist eine eindimensionale Datenstruktur, die mit einem NumPy-Array vergleichbar ist. Eine Series kann mit der Funktion pd.Series() erstellt werden. Als Argument wird ein Array übergeben. Die Series hat einen Index, der standardmäßig mit 0 beginnt. Der Index kann mit dem Argument index angepasst werden. Der Index kann ein Array von Strings sein, um die Elemente der Series zu benennen.\n# Erstellen einer Series\ns = pd.Series([1, 2, 3, 4, 5])\ns\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n# Erstellen einer Series mit Index\ns1 = pd.Series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e'])\ns1\n\na    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64\nDie Series hat eine Reihe von Methoden, die aufgerufen werden können. Eine Übersicht über die Methoden kann mit help(pd.Series) aufgerufen werden. Die wichtigsten Methoden sind:\n# Zugriff auf den Index\ns1.index\n\nIndex(['a', 'b', 'c', 'd', 'e'], dtype='object')\n# Zugriff auf die Werte\ns1.values\n\narray([1, 2, 3, 4, 5])\n# Zugriff auf ein Element\ns1['a']\n\n1\n# Zugriff auf mehrere Elemente\ns1[['a', 'c', 'e']]\n\na    1\nc    3\ne    5\ndtype: int64\n# Zugriff auf ein Element mit dem Ganzzahl-Index\ns1.iloc[0]\n\n1\n# Zugriff auf mehrere Elemente mit dem Index\ns1.loc[\"a\"]\n\n1\ns1.loc[[\"a\", \"c\", \"e\"]]\n\na    1\nc    3\ne    5\ndtype: int64\ns1.sum()\n\n15\ns1.mean()\n\n3.0\ns1.std()\n\n1.5811388300841898",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "01-Python/01-04-Pandas.html#dataframe",
    "href": "01-Python/01-04-Pandas.html#dataframe",
    "title": "4  Pandas",
    "section": "4.2 DataFrame",
    "text": "4.2 DataFrame\nDas wichtigste Datenobjekt in Pandas ist der DataFrame. Ein DataFrame ist eine zweidimensionale Datenstruktur, die mit einer Tabelle vergleichbar ist. Ein DataFrame kann mit der Funktion pd.DataFrame() erstellt werden. Als Argument wird ein Array übergeben. Der DataFrame hat einen Index, der standardmäßig mit 0 beginnt. Der Index kann mit dem Argument index angepasst werden. Der Index kann ein Array von Strings sein, um die Zeilen des DataFrame zu benennen. Die Spalten des DataFrame können mit dem Argument columns benannt werden.\n\ndf = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 7, 9], [10, 11, 12]], columns=['A', 'B', 'C'])\ndf\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n2\n3\n\n\n1\n4\n5\n6\n\n\n2\n7\n7\n9\n\n\n3\n10\n11\n12\n\n\n\n\n\n\n\n\n\ndf1 = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 7, 9], [10, 11, 12]],\n                   index=['a', 'b', 'c', 'd'],\n                   columns=['X', 'Y', 'Z']\n                   )\ndf1\n\n\n\n\n\n\n\n\n\nX\nY\nZ\n\n\n\n\na\n1\n2\n3\n\n\nb\n4\n5\n6\n\n\nc\n7\n7\n9\n\n\nd\n10\n11\n12\n\n\n\n\n\n\n\n\nDie DataFrame hat eine Reihe von Methoden, die aufgerufen werden können. Eine Übersicht über die Methoden kann mit help(pd.DataFrame) aufgerufen werden. Die wichtigsten Methoden sind:\n\ndf1[\"X\"].sum()\n\n22\n\n\n\ndf1[\"X\"].mean()\n\n5.5\n\n\n\ndf1[\"X\"].std()\n\n3.872983346207417\n\n\n\ndf1[\"X\"].describe()\n\ncount     4.000000\nmean      5.500000\nstd       3.872983\nmin       1.000000\n25%       3.250000\n50%       5.500000\n75%       7.750000\nmax      10.000000\nName: X, dtype: float64",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "01-Python/01-04-Pandas.html#axes",
    "href": "01-Python/01-04-Pandas.html#axes",
    "title": "4  Pandas",
    "section": "4.3 Axes",
    "text": "4.3 Axes\nDie axes eines DataFrame sind die Zeilen und Spalten des DataFrame. Die Zeilen können mit df.index und die Spalten mit df.columns abgerufen werden. Die Eigenschaft (property) shape gibt die Anzahl der Zeilen und Spalten des DataFrame zurück.\n\ndf1.shape\n\n(4, 3)\n\n\nViele Methoden können auf die Zeilen und Spalten des DataFrame angewendet werden.\n\ndf1\n\n\n\n\n\n\n\n\n\nX\nY\nZ\n\n\n\n\na\n1\n2\n3\n\n\nb\n4\n5\n6\n\n\nc\n7\n7\n9\n\n\nd\n10\n11\n12\n\n\n\n\n\n\n\n\n\ndf1.mean(axis=0)\n\nX    5.50\nY    6.25\nZ    7.50\ndtype: float64\n\n\n\ndf1.mean(axis=1)\n\na     2.000000\nb     5.000000\nc     7.666667\nd    11.000000\ndtype: float64\n\n\n\ndf1.mean()\n\nX    5.50\nY    6.25\nZ    7.50\ndtype: float64",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "01-Python/01-04-Pandas.html#neue-spalte-hinzufügen",
    "href": "01-Python/01-04-Pandas.html#neue-spalte-hinzufügen",
    "title": "4  Pandas",
    "section": "4.4 Neue Spalte hinzufügen",
    "text": "4.4 Neue Spalte hinzufügen\nEine neue Spalte kann zu einem DataFrame hinzugefügt werden, indem ein neues Array als Wert der neuen Spalte zugewiesen wird. Die neue Spalte wird automatisch an das Ende des DataFrame hinzugefügt.\n\ndf1[\"New\"] = [1, 2, 3, 4]\ndf1.head()\n\n\n\n\n\n\n\n\n\nX\nY\nZ\nNew\n\n\n\n\na\n1\n2\n3\n1\n\n\nb\n4\n5\n6\n2\n\n\nc\n7\n7\n9\n3\n\n\nd\n10\n11\n12\n4\n\n\n\n\n\n\n\n\nHäufig wird eine neue Spalte aus bestehenden Spalten berechnet. Dazu können die bestehenden Spalten wie normale Arrays verwendet werden. Die neue Spalte wird automatisch an das Ende des DataFrame hinzugefügt.\n\ndf1[\"X_min_Y\"] = df1[\"X\"] - df1[\"Y\"]\ndf1.head()\n\n\n\n\n\n\n\n\n\nX\nY\nZ\nNew\nX_min_Y\n\n\n\n\na\n1\n2\n3\n1\n-1\n\n\nb\n4\n5\n6\n2\n-1\n\n\nc\n7\n7\n9\n3\n0\n\n\nd\n10\n11\n12\n4\n-1\n\n\n\n\n\n\n\n\n\ndf1[\"X_2\"] = 2 *df1[\"X\"]\ndf1.head()\n\n\n\n\n\n\n\n\n\nX\nY\nZ\nNew\nX_min_Y\nX_2\n\n\n\n\na\n1\n2\n3\n1\n-1\n2\n\n\nb\n4\n5\n6\n2\n-1\n8\n\n\nc\n7\n7\n9\n3\n0\n14\n\n\nd\n10\n11\n12\n4\n-1\n20",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "01-Python/01-04-Pandas.html#selektieren-von-zeilen",
    "href": "01-Python/01-04-Pandas.html#selektieren-von-zeilen",
    "title": "4  Pandas",
    "section": "4.5 Selektieren von Zeilen",
    "text": "4.5 Selektieren von Zeilen\nHäufig möchten wir nur eine Teilmenge der Zeilen eines DataFrame selektieren, z.B. die ersten 5 Zeilen oder die Zeilen, die eine bestimmte Bedingung erfüllen (z.B. Männer/Frauen, beschäftigt/arbeitslos, etc.).\n\n# Als Beispiel wählen wir die ersten drei Zeilen\n\ndf1.iloc[0:2]\n\n\n\n\n\n\n\n\n\nX\nY\nZ\nNew\nX_min_Y\nX_2\n\n\n\n\na\n1\n2\n3\n1\n-1\n2\n\n\nb\n4\n5\n6\n2\n-1\n8\n\n\n\n\n\n\n\n\n\n# Als anderes Beispiel wählen wir die Zeilen, für die X größer als 3 ist\n\ndf1[df1[\"X\"] &gt; 3]\n\n\n\n\n\n\n\n\n\nX\nY\nZ\nNew\nX_min_Y\nX_2\n\n\n\n\nb\n4\n5\n6\n2\n-1\n8\n\n\nc\n7\n7\n9\n3\n0\n14\n\n\nd\n10\n11\n12\n4\n-1\n20",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "01-Python/01-04-Pandas.html#selektieren-von-spalten",
    "href": "01-Python/01-04-Pandas.html#selektieren-von-spalten",
    "title": "4  Pandas",
    "section": "4.6 Selektieren von Spalten",
    "text": "4.6 Selektieren von Spalten\nHäufig möchten wir nur eine Teilmenge der Spalten eines DataFrame selektieren, z.B. die Spalten, die für eine bestimmte Analyse relevant sind.\n\ndf1[[\"X\", \"Y\"]]\n\n\n\n\n\n\n\n\n\nX\nY\n\n\n\n\na\n1\n2\n\n\nb\n4\n5\n\n\nc\n7\n7\n\n\nd\n10\n11",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "01-Python/01-04-Pandas.html#groupby",
    "href": "01-Python/01-04-Pandas.html#groupby",
    "title": "4  Pandas",
    "section": "4.7 GroupBy",
    "text": "4.7 GroupBy\nMit der Funktion groupby() können die Daten in einem DataFrame gruppiert werden. Die Funktion groupby() gibt ein DataFrameGroupBy-Objekt zurück. Mit diesem Objekt können verschiedene Aggregationsfunktionen aufgerufen werden. Die wichtigsten Aggregationsfunktionen sind: - count(): Anzahl der Elemente - sum(): Summe der Elemente - mean(): Mittelwert der Elemente - median(): Median der Elemente - min(): Minimum der Elemente - max(): Maximum der Elemente - std(): Standardabweichung der Elemente - var(): Varianz der Elemente - describe(): Statistische Kennzahlen der Elemente - first(): Erstes Element - last(): Letztes Element - nth(): n-tes Element\n\ndf2 = pd.DataFrame({\n    \"Name\": [\"Alice\", \"Bob\", \"Mallory\", \"Mallory\", \"Bob\", \"Mallory\"],\n    \"Gender\": [\"f\", \"m\", \"f\", \"f\", \"m\", \"f\"],\n    \"YearOfBirth\": [1999, 1985, 1997, 1990, 1987, 1990],\n}\n)\ndf2[\"Age\"] = 2024 - df2[\"YearOfBirth\"]\ndf2.head()\n\n\n\n\n\n\n\n\n\nName\nGender\nYearOfBirth\nAge\n\n\n\n\n0\nAlice\nf\n1999\n25\n\n\n1\nBob\nm\n1985\n39\n\n\n2\nMallory\nf\n1997\n27\n\n\n3\nMallory\nf\n1990\n34\n\n\n4\nBob\nm\n1987\n37\n\n\n\n\n\n\n\n\n\ndf2_by_gender = df2.groupby(\"Gender\")\ndf2_by_gender\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f591116e650&gt;\n\n\n\ndf2_by_gender[\"Age\"].mean()\n\nGender\nf    30.0\nm    38.0\nName: Age, dtype: float64",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "01-Python/01-05-Pandas-Ex.html",
    "href": "01-Python/01-05-Pandas-Ex.html",
    "title": "5  Übung zu Pandas",
    "section": "",
    "text": "Für den Zweck der Buchhaltung in einem Unternehmen soll ein DataFrame erstellt werden, welches die folgenden Spalten enthält:\n\nPosition: Die Position im Unternehmen\nArbeitszeit: Die Arbeitszeit pro Woche\nLohn: Der wöchentliche Lohn in Euro\n\nDie Werte für die Position sollen aus der folgenden Liste entnommen werden:\nPosition: ['CEO', 'CFO', 'CTO', 'CIO', 'COO', 'CDO', 'CMO', 'CRO', 'CSO', 'CPO']\nArbeitszeit: [40, 40, 20, 40, 20, 40, 10, 40, 40, 60]\nGehalt: [10000, 8500, 2000, 3022, 1039, 2500, 3000, 1800, 20000, 10000]\n\n6 Aufgabe 1\n\nNennen Sie den DataFrame df und geben Sie ihn aus.\nGeben Sie die Spalte Position aus.\nGeben Sie die ersten 3 Zeilen des DataFrames aus.\nGeben Sie die letzten 3 Zeilen des DataFrames aus.\nGeben Sie die ersten 3 Zeilen der Spalte Position aus.\nGeben Sie die letzten 3 Zeilen der Spalte Position aus.\nGeben Sie die ersten 3 Zeilen der Spalten Position und Gehalt aus.\n\n\n\n7 Aufgabe 2\n\nBerechnen Sie den durchschnittlichen Gehalt im Unternehmen.\nBerechnen Sie den Stundenlohn für jede Personen im Unternehmen.\nBerechnen Sie den durchschnittlichen Stundenlohn im Unternehmen.\nBerechnen Sie den durchschnittlichen Stundenlohn für jede Position im Unternehmen.\nFinden Sie den niedrigsten Stundenlohn im Unternehmen.\nFinden Sie die Position mit dem niedrigsten Stundenlohn im Unternehmen.\nFinden Sie den höchsten Stundenlohn im Unternehmen.",
    "crumbs": [
      "Einführung in Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Übung zu Pandas</span>"
    ]
  },
  {
    "objectID": "02-Deskriptive-Statistik.html",
    "href": "02-Deskriptive-Statistik.html",
    "title": "6  Deskriptive Statistik",
    "section": "",
    "text": "6.1 Notation\nEine Variable ist eine Eigenschaft, die wir an einer Person, einem Objekt oder einem Ereignis messen können. In diesem Beispiel ist die Variable height die Größe einer Person. Im allgemeinen werden wir solche Variablen mit mit lateinischen Buchstaben bezeichnen, z.B. x, y, z. Was diese Bezeichnungen bedeuten, hängt vom Kontext ab. In diesem Beispiel sei x die Größe der Personen aus der Erhebung. Wir werden die Beobachtungen (hier Personen) üblicherweise mit einem Index bezeichnen: i = 1,2, \\ldots, n, wobei n die Anzahl der Beobachtungen ist. Die Werte der Variable x für die Beobachtungen i = 1,2, \\ldots, n bezeichnen wir mit x_1, x_2, \\ldots, x_n.\nZum Beispiel ist die Größe des zweiten Besuchers x_2 = 66 Inch.\n# Extract the height of the second customer\n# Remember that the index is 0-based, so the second customer has index 1\ncustomers[\"height\"][1]\n\n66\n# Extract the heights of the first three customers\n# Remember that indexing is exclusive, so the last index is not included\ncustomers[\"height\"][:3]\n\n0    74\n1    66\n2    64\nName: height, dtype: int64\n# Calculate the average height of the first three customers\ncustomers[\"height\"][:3].mean()\n\n68.0\nNun möchten wir den Mittelwert der Größe für alle Besucher berechnen.\nprint(customers[\"height\"].mean())\nprint(np.mean(customers[\"height\"]))\n\n66.56883259911895\n66.56883259911895",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "02-Deskriptive-Statistik.html#notation",
    "href": "02-Deskriptive-Statistik.html#notation",
    "title": "6  Deskriptive Statistik",
    "section": "",
    "text": "Definition 6.1 (Stichprobenmittelwert (Sample Mean)) \n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\nDas ist einfach die Summe aller Werte geteilt durch die Anzahl der Werte.\n\n\\bar{x} = \\frac{x_1 + x_2 + \\ldots + x_n}{n}\n\n\n\nBeispiel 6.1 (Stichprobenmittelwert) Lasst uns den Mittelwert der ersten drei Besucher ausrechnen: x_1 = 74, x_2 = 66, x_3 = 64.\n\n\\frac{74 + 66 + 64}{3} = 68",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "02-Deskriptive-Statistik.html#streuung",
    "href": "02-Deskriptive-Statistik.html#streuung",
    "title": "6  Deskriptive Statistik",
    "section": "6.2 Streuung",
    "text": "6.2 Streuung\nDer Mittelwert informiert uns über das Zentrum (Lage) der Daten. Die zweite Charakteristik einer Variable, die wir verstehen wollen, ist wie weit die Daten verteilt sind (wie unterschiedlich, heterogen, die Kunden in Bezug auf die Größe sind, zum Beispiel). Der Begriff der Streuung ist abstrakt und kann auf verschiedene Weisen gemessen werden. Wir können uns die Streuung zum Beispiel als die durchschnittliche Differenz der Kunden zum Mittelwert vorstellen. Dies ist das Konzept der Varianz.\n\nDefinition 6.2 (Stichprobenvarianz und Stichprobenstandardabweichung) Für eine Variable x mit n Beobachtungen: x_1, x_2, \\ldots, x_n, ist die Stichprobenvarianz definiert als:\n\nS_x^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\nDabei ist \\bar{x} der Stichprobenmittelwert.\nDie Stichprobenstandardabweichung ist die Quadratwurzel der Stichprobenvarianz:\n\nS_x = \\sqrt{S^2}\n\n\n\nBeispiel 6.2 (Stichprobenvarianz und Stichprobenstandardabweichung) Lasst uns die Stichprobenstandardabweichung für die ersten drei Besucher berechnen x_1 = 74, x_2 = 66, x_3 = 64.\nZuerst müssen wir den Stichprobenmittelwert ausrechnen: \\bar{x} = 68 (Beispiel 6.1)\nDanach berechnen wir die Differenzen der Werte zum Mittelwert und quadrieren diese:\n\nS^2 = \\frac{(74 - 68)^2 + (66 - 68)^2 + (64 - 68)^2}{3-1} = \\frac{36 + 4 + 16}{2} = 28\n\nDie Stichprobenstandardabweichung ist die Quadratwurzel der Stichprobenvarianz:\n\nS = \\sqrt{28} \\approx 5.29\n\n\n\n# Hier replizieren wir die Berechnung der Varianz der Größe der ersten drei Kunden\ncustomers[\"height\"][0:3].var()\n\n28.0\n\n\n\n# Sie können die Varianz auch mit der numpy Funktion np.var berechnen\n\nnp.var(customers[\"height\"][0:3])\n\n18.666666666666668\n\n\n\nnp.var(customers[\"height\"][0:3], ddof=1)\n\n28.0\n\n\n\n# Die Standardabweichung der Größe der ersten drei Besucher berechnen\n\ncustomers[\"height\"][0:3].std()\n\n5.291502622129181\n\n\n\nnp.std(customers[\"height\"][0:3], ddof=1)\n\n5.291502622129181\n\n\nnp.var und np.std berechnen die Stichprobenvarianz und -standardabweichung nach der Formel\n\nS^2 = \\frac{1}{1 - \\text{ddof}} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\nwobei ddof die Anzahl der Freiheitsgrade im Nenner (denominator degrees of freedom) steht. Die Voreinstellung ist ddof=0.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "02-Deskriptive-Statistik.html#weitere-maße-der-lage-und-streuung",
    "href": "02-Deskriptive-Statistik.html#weitere-maße-der-lage-und-streuung",
    "title": "6  Deskriptive Statistik",
    "section": "6.3 Weitere Maße der Lage und Streuung",
    "text": "6.3 Weitere Maße der Lage und Streuung\nDie Reduzierung auf einen einzigen Wert als Stichprobenmittelwert ist sehr nützlich, weil wir leicht darüber nachdenken können (nur ein Wert). Allerdings ignoriert diese Reduktion viele Details über die Kunden. Zum Beispiel wissen wir nicht, wie viele Kunden nahe am Mittelwert sind, wie viele weit entfernt sind, und so weiter. Die Standardabweichung hilft uns, dies zu verstehen, ist aber immer noch eine einzige Zahl.\nUm unser Verständnis der Daten weiter zu vertiefen, können wir andere Maße der Lage und Streuung verwenden.\n\nDer Median ist der Wert, der die Daten in zwei gleiche Hälften teilt. Es ist der Wert, der die gleiche Anzahl von Beobachtungen über und unter sich hat. Sie können sagen, dass etwa die Hälfte der Beobachtungen einen Wert kleiner als den Median hat und etwa die andere Hälfte einen Wert größer als den Median hat. Ein anderer Name für den Median ist das zweite Quartil (Q2). Dies kommt daher, dass die Stichprobe in vier Teile geteilt wird. Ein weiterer Name für den Median ist das 50. Perzentil (Teilung der Stichprobe in 100 Teile) oder das 0,5-Quantil.\nDas erste Quartil (Q1) ist der Wert, der die ersten 25% der Daten von den restlichen trennt. Sie können sagen, dass ungefähr 25% der Beobachtungen einen Wert kleiner als Q1 und 75% einen Wert größer als Q1 haben. Ein anderer Name für das erste Quartil ist das 25. Perzentil oder das 0,25-Quantil.\nDas zweite Quartil (Q2) ist der Median.\nDas dritte Quartil (Q3) ist der Wert, der die ersten 75% der Daten von den restlichen trennt. Sie können sagen, dass ungefähr 75% der Beobachtungen einen Wert kleiner als Q3 und 25% einen Wert größer als Q3 haben. Ein anderer Name für das dritte Quartil ist das 75. Perzentil oder das 0,75-Quantil.\nDas Maximum ist der größte Wert in den Daten. Es wird manchmal das 100. Perzentil genannt.\nDas Minimum ist der kleinste Wert in den Daten. Es wird manchmal das 0. Perzentil genannt.\n\nDie Differenz zwischen dem Minimum und dem Maximum ist der Bereich (range) der Daten. Es ist ein Maß für die Streuung der Daten, aber es ist sehr anfällig für extreme Werte. Ein Ausreißer ist ein Wert, der sehr weit von den anderen Werten entfernt ist. Der Median und die Quartile sind weniger anfällig für Ausreißer und werden als robuste Maße der Lage und Streuung bezeichnet.\nDie Differenz zwischen dem dritten und dem ersten Quartil ist der Interquartilsabstand (IQR). Es ist ein Maß für die Streuung der mittleren 50% der Daten. Es ist auch ein robustes Maß der Streuung, da es weniger anfällig für Ausreißer ist.\n\n# Mittelwert, Standardabweichung, Minimum und Maximum und die Quartile der Größe der Kunden berechnen\n\ncustomers[\"height\"].describe()\n\ncount    1816.000000\nmean       66.568833\nstd         3.831822\nmin        57.000000\n25%        64.000000\n50%        66.000000\n75%        69.250000\nmax        82.000000\nName: height, dtype: float64\n\n\n\n# Falls wir nur eine der statistischen Kennzahlen berechnen wollen, können wir auch die entsprechende Funktion verwenden\n\nprint(customers[\"height\"].min())\nprint(customers[\"height\"].quantile(0.25))\nprint(customers[\"height\"].quantile(0.5))\nprint(customers[\"height\"].quantile(0.75))\nprint(customers[\"height\"].max())\n\n57\n64.0\n66.0\n69.25\n82\n\n\n\n# Den Interquartilsabstand berechnen\n\ncustomers[\"height\"].quantile(0.75) - customers[\"height\"].quantile(0.25)\n\n5.25\n\n\nBerechnen Sie den Mittelwert, Median, Standardabweichung, Minimum, Maximum, Quartile und Interquartilsabstand für das Gewicht der Kunden.\n\n# Schreiben Sie Ihren Code hier\n\nHäufig werden diese Quantile und der Interquartilsabstand in einem Boxplot dargestellt. Ein Boxplot ist ein Diagramm, das die Verteilung der Daten in einem Box-Whisker-Diagramm darstellt. Die Box repräsentiert das Interquartilsintervall (IQR). Die Whisker repräsentieren die Daten außerhalb des IQR. Punkte, die außerhalb der Whisker liegen sind Punkte, die weit von den anderen Daten entfernt sind und werden Ausreißer genannt (was ein Ausreißer ist, ist allerdings kompliziert).\n\nsns.boxplot(customers[\"height\"], orient=\"h\")\n\n\n\n\n\n\n\n\nDie Boxplots sind nützlich, wenn wir die Verteilung einer Variablen in verschiedenen Gruppen vergleichen wollen. Als Beispiel können wir die Verteilung der Größe in verschiedenen ethnischen Gruppen vergleichen.\n\nsns.boxplot(x = customers[\"height\"], y = customers[\"ethnicity\"])",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "02-Deskriptive-Statistik.html#zusammenfassung-kategorialer-variablen",
    "href": "02-Deskriptive-Statistik.html#zusammenfassung-kategorialer-variablen",
    "title": "6  Deskriptive Statistik",
    "section": "6.4 Zusammenfassung kategorialer Variablen",
    "text": "6.4 Zusammenfassung kategorialer Variablen\nDer Mittelwert und die Standardabweichung machen nur für Variablen Sinn, für die Addition und Multiplikation sinnvoll sind. Insbesondere machen sie keinen Sinn für kategoriale Variablen, z. B. die Ethnie der Kunden.\nDie wichtigste Zusammenfassung für kategoriale Variablen ist die Häufigkeitstabelle. Eine Häufigkeitstabelle zeigt die Anzahl der Beobachtungen für jede Kategorie. Die relative Häufigkeitstabelle zeigt den Anteil der Beobachtungen für jede Kategorie.\n\ncustomers[\"ethnicity\"].value_counts()\n\nethnicity\nWhite       1494\nBlack        180\nHispanic     104\nOther         38\nName: count, dtype: int64\n\n\n\nsns.countplot(data = customers, x=\"ethnicity\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "02-Deskriptive-Statistik.html#datentransformationen",
    "href": "02-Deskriptive-Statistik.html#datentransformationen",
    "title": "6  Deskriptive Statistik",
    "section": "6.5 Datentransformationen",
    "text": "6.5 Datentransformationen\nEs ist in fast jeder Analyse notwendig, die Daten zu transformieren. Als Beispiel können wir die Größe in Inch in Zentimeter umrechnen. Die Umrechnung von Inch in Zentimeter erfolgt durch Multiplikation mit 2,54. Die Umrechnung von Pfund in Kilogramm erfolgt durch Multiplikation mit 0,45.\n\n# Hier erstellen wir eine neue Spalte, die die Größe in cm enthält\ncustomers[\"height_cm\"] = customers[\"height\"] * 2.54\n\n\n# Erstellen Sie eine neue Spalte \"weight_kg\" in dem Dataframe \"customers\", die die Werte der Spalte \"weight\" in Kilogramm umrechnet.\n# 1 Pfund entspricht 0.453592 Kilogramm.\n\nHäufig möchten wir eine stetige Variable in Kategorien einteilen. Zum Beispiel möchten wir eine Variable erstellen, die angibt, ob eine Person jünger als 30 Jahre ist oder nicht. Dies ist eine binäre Variable, die wir aus einer kontinuierlichen Variable erstellen.\n\ncustomers[\"is_under_30\"] = customers[\"age\"] &lt; 30\ncustomers[\"is_under_30\"].head()\n\n0    False\n1    False\n2     True\n3    False\n4    False\nName: is_under_30, dtype: bool\n\n\nFalls wir mehr als zwei Kategorien brauchen, bietet die Funktion pd.cut die Möglichkeit, die Daten in Kategorien einzuteilen.\n\n# Wir lassen uns die Häufigkeitsverteilung der Altersgruppen ausgeben,\n# um zu sehen, wie pd.cut funktioniert\n\npd.cut(\n    customers[\"age\"],\n    bins=[0, 30, 64, np.inf],\n).value_counts()\n\nage\n(30.0, 64.0]    1041\n(0.0, 30.0]      512\n(64.0, inf]      263\nName: count, dtype: int64\n\n\n\n# Hier weisen wir Etiketten der Kategorien und speichern das Ergebnis in einer neuen Spalte \"age_group\"\n\ncustomers[\"age_group\"] = pd.cut(\n    customers[\"age\"],\n    bins=[0, 30, 64, np.inf],\n    labels=[\"&lt;30\", \"30-64\", \"65+\"]\n)\n\n# Als Kontrolle lassen wir uns die ersten 5 Zeilen des Dataframes anzeigen\ncustomers[[\"age\", \"age_group\"]].head()\n\n\n\n\n\n\n\n\n\nage\nage_group\n\n\n\n\n0\n45\n30-64\n\n\n1\n58\n30-64\n\n\n2\n29\n&lt;30\n\n\n3\n57\n30-64\n\n\n4\n91\n65+",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "02-Deskriptive-Statistik.html#aufgabe",
    "href": "02-Deskriptive-Statistik.html#aufgabe",
    "title": "6  Deskriptive Statistik",
    "section": "6.6 Aufgabe",
    "text": "6.6 Aufgabe\nBerechnen Sie den BMI (Body Mass Index) für die Kunden. Der BMI ist definiert als das Körpergewicht in Kilogramm geteilt durch das Quadrat der Größe in Metern. Das Gewicht muss in Kilogramm und die Größe in Metern sein.\n\n# Erstellen Sie die Spalte \"bmi\" in dem Dataframe \"customers\", die den Body Mass Index berechnet.\n\ncustomers[\"bmi\"] = customers[\"weight\"] / (customers[\"height\"] / 100) ** 2\n\nDie Referenzwerte für den BMI sind:\n\nUnter 18,5: Untergewicht\n18,5 - 24,9: Normalgewicht\n25 - 29,9: Übergewicht\n30 oder mehr: Adipositas\nEin Verkäufer behauptet, daß die Kunden im Einkaufszentrum im Durchschnitt übergewichtig sind. Überprüfen Sie diese Behauptung anhand der Daten.\nEin anderer Verkäufer behauptet, daß mehr als ein Viertel der Besucher im Einkaufszentrum adipös sind. Überprüfen Sie diese Behauptung anhand der Daten.\n\n\n# Berechnen Sie den durchschnittlichen BMI\n\ncustomers[\"bmi\"].mean()\n\n351.1234041165582\n\n\n\n# Lassen Sie sich die Beschreibung der Spalte \"bmi\" anzeigen. (Tipp: Verwenden Sie die Methode .describe() der Spalte \"bmi\").\n\ncustomers[\"bmi\"].describe()\n\ncount    1789.000000\nmean      351.123404\nstd        65.866606\nmin       163.265306\n25%       307.478675\n50%       340.136054\n75%       378.703497\nmax       761.862330\nName: bmi, dtype: float64\n\n\nErstellen Sie eine neue Spalte is_underweight in der Tabelle customers, die angibt, ob ein Kunde untergewichtig ist oder nicht. Erstellen Sie eine weitere Spalte is_normal in der Tabelle customers, die angibt, ob ein Kunde ein Normalgewicht hat oder nicht.\nHinweis: Sie können diese Variable mit den logischen Operatoren &lt;, &lt;=, &gt;, &gt;= und == erstellen. Logische Operationen können Sie mit den Operatoren & (und), | (oder) und ~ (nicht) verknüpfen (Hinweis: hier brauchen Sie und).\n\n# Erstellen Sie die zwei Variablen hier\n\ncustomers[\"is_overweight\"] = customers[\"bmi\"] &gt; 25\n\nWie viele Kunden sind untergewichtig? Wie viele Kunden haben ein Normalgewicht? Hinweis: Benutzen Sie die Methode value_counts um die Anzahl der Kunden in jeder Kategorie zu berechnen. Sie können auch die Methode sum benutzen, um die Anzahl der Kunden in einer Kategorie zu berechnen.\n\n# Wie viele Kunden sind untergewichtig?\n# a) mit .value_counts()\n\n# b) mit .sum()\n\nErstellen Sie eine neue Spalte bmi_category in der Tabelle customers, die die Kategorie des BMI angibt (Untergewicht, Normalgewicht, Übergewicht, Adipositas). Hinweis: Sie können diese Variable mit der Funktion pd.cut erstellen.\n\n# Erstellen Sie `bmi_category` hier\n\nVisualisieren Sie die Verteilung der Kategorien des BMI. Hinweis: Benutzen Sie die Funktion sns.countplot aus der Bibliothek seaborn.\n\n# Erstellen Sie die Graphik hier\n\nNun möchten Wir die Verteilung des BMI für Männer und Frauen vergleichen. Berechnen Sie den Mittelwert des BMI für Männer und Frauen. Hinweis: Sie können die Methode groupby benutzen, um die Mittelwerte für verschiedene Gruppen zu berechnen.\n\n# Als Beispiel für die Benutzung von .groupby() werden hier die Mittelwerte des Alters für jede Ethnie berechnet\n\ncustomers.groupby(\"ethnicity\")[\"age\"].mean()\n\nethnicity\nBlack       41.733333\nHispanic    36.288462\nOther       41.763158\nWhite       43.571620\nName: age, dtype: float64\n\n\n\n# Berechnen Sie die Mittelwerte des BMI für Männer und Frauen.\n\n\n# Vergleichen Sie die Verteilungen des BMI für Männer und Frauen mit einem Boxplot.\n\n# Bevor wir die Boxplots erstellen, müssen wir die Variable \"male\" in eine kategorische Variable umwandeln. Die neue Variable nenne wir \"sex\"\ncustomers[\"sex\"] = pd.Categorical(customers[\"male\"], ordered=False)\n\n# Erstellen Sie die Boxplots hier",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "03-KQ-Methode.html",
    "href": "03-KQ-Methode.html",
    "title": "7  Die KQ-Methode",
    "section": "",
    "text": "7.1 KQ-Methode im Modell nur mit einer Konstante\nZuerst werden wir das Problem in einem sehr einfachen Fall betrachten. Betrachten wir die Prognosegleichung\n\\hat{y} = \\hat{\\beta}_0\nDie Prognose ist für alle Werte von x gleich \\hat{\\beta}_0 (d.h. die Prognose ist eine horizontale Gerade). Wo sollten wir diese horizontale Gerade zeichnen, um die Residuenquadratsumme zu minimieren?\nFür die Gleichung oben ist die Residuenquadratsumme:\n\\text{RSS}(\\hat{\\beta}_0) = \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0)^2\nIn Abbildung 7.5 ist die RSS für mehrere Werte von \\hat{\\beta}_0 zwischen 1.1 und 3.1 dargestellt.\nCode\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nbeta0 = np.linspace(2.1 - 1, 2.1 + 1, 100)\nrss_beta0 = np.zeros_like(beta0)\n\nfor i in range(len(beta0)):    \n    res = invoices['Time'] - beta0[i]\n    rss_beta0[i] = np.sum(res**2)\n\nax.plot(beta0, rss_beta0)\nax.set_xlabel(r\"$\\hat{\\beta}_0$\")\nax.set_ylabel(r\"$RSS(\\hat{\\beta}_0)$\")\n\n\n\n\n\n\n\nText(0, 0.5, '$RSS(\\\\hat{\\\\beta}_0)$')\n\n\n(a) r”RSS für verschiedene Werte \\hat{\\beta}_0 im Modell \\widehat{Time} = \\hat{\\beta}_0.”\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nAbbildung 7.5",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Die KQ-Methode</span>"
    ]
  },
  {
    "objectID": "03-KQ-Methode.html#kq-methode-im-modell-nur-mit-einer-konstante",
    "href": "03-KQ-Methode.html#kq-methode-im-modell-nur-mit-einer-konstante",
    "title": "7  Die KQ-Methode",
    "section": "",
    "text": "Übungsaufgabe 7.3 (Das Modell nur mit einer Konstante) Leiten Sie den Wert von \\hat{\\beta}_0 her, der die Residuenquadratsumme minimiert.\n\n\n\n\n\n\n\nBeweis\n\n\n\n\n\nWir fangen mit der ersten Ableitung der Residuenquadratsumme nach \\hat{\\beta}_0 an.\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\hat{\\beta}_0} RSS(\\hat{\\beta}_0) & =\n\\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0) \\cdot (-1) \\\\\n& = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0) \\\\\n& = -2 \\sum_{i=1}^n y_i + 2 \\sum_{i=1}^n \\hat{\\beta}_0 \\\\\n& = -2 \\sum_{i=1}^n y_i + 2 n \\hat{\\beta}_0\n\\end{align*}\n\nDie notwendige Bedingung für ein Extremum (Minimum oder Maximum) ist, daß die erste Ableitung gleich Null ist.\n\n\\begin{align*}\n-2 \\sum_{i=1}^n y_i + 2 n \\hat{\\beta}_0 & = 0 \\\\\n\\hat{\\beta}_0 & = \\frac{1}{n} \\sum_{i=1}^n y_i \\\\\n& = \\overline{y}\n\\end{align*}\n\nDer optimale Wert von \\hat{\\beta}_0, der die Residuenquadratsumme minimiert, ist einfach der Durchschnitt der beobachteten Werte.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Die KQ-Methode</span>"
    ]
  },
  {
    "objectID": "03-KQ-Methode.html#das-modell-mit-einer-variable-ohne-konstante",
    "href": "03-KQ-Methode.html#das-modell-mit-einer-variable-ohne-konstante",
    "title": "7  Die KQ-Methode",
    "section": "7.2 Das Modell mit einer Variable ohne Konstante",
    "text": "7.2 Das Modell mit einer Variable ohne Konstante\nLasst uns jetzt das Modell mit einer Variable betrachten, aber ohne Konstante. Die Prognosegleichung ist\n\n\\hat{y} = \\hat{\\beta}_1 x\n\nLeiten Sie den Wert von \\hat{\\beta}_1 her, der die Residuenquadratsumme minimiert.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Die KQ-Methode</span>"
    ]
  },
  {
    "objectID": "03-KQ-Methode.html#herleitung",
    "href": "03-KQ-Methode.html#herleitung",
    "title": "7  Die KQ-Methode",
    "section": "7.3 Herleitung",
    "text": "7.3 Herleitung\nWir fangen mit der ersten Ableitung der Residuenquadratsumme nach \\hat{\\beta}_1 an.\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\hat{\\beta}_1} RSS(\\hat{\\beta}_1) & =\n\\sum_{i=1}^n 2(y_i - \\hat{\\beta}_1 x_i) \\cdot (-x_i) \\\\\n& = -2 \\sum_{i=1}^n x_i y_i + 2 \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2\n\\end{align*}\n\nDie notwendige Bedingung für ein Extremum (Minimum oder Maximum) ist, daß die erste Ableitung gleich Null ist.\n\n\\begin{align*}\n-2 \\sum_{i=1}^n x_i y_i + 2 \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = 0 \\\\\n\\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n x_i y_i \\\\\n\\hat{\\beta}_1 & = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2} \\\\\n& = \\frac{\\frac{1}{n}\\sum_{i=1}^n x_i y_i}{\\frac{1}{n}\\sum_{i=1}^n x_i^2} \\\\\n& = \\frac{\\overline{x y}}{\\overline{x^2}}\n\\end{align*}",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Die KQ-Methode</span>"
    ]
  },
  {
    "objectID": "03-KQ-Methode.html#das-modell-mit-einer-variable-und-einer-konstante",
    "href": "03-KQ-Methode.html#das-modell-mit-einer-variable-und-einer-konstante",
    "title": "7  Die KQ-Methode",
    "section": "7.4 Das Modell mit einer Variable und einer Konstante",
    "text": "7.4 Das Modell mit einer Variable und einer Konstante\nIm Modell mit einer Konstante und einem Predictor ist die Prognosegleichung \n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n\nDie RSS ist\n\n\\begin{align*}\n\\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n  & = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\n\\end{align*}\n\nDie notwendige Bedingung für ein Extremum (Minimum oder Maximum) ist, dass die erste Ableitung gleich Null ist.\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\hat{\\beta}_0} \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\\n\\frac{\\partial}{\\partial \\hat{\\beta}_1} \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) x_i = 0\n\\end{align*}\n\nAus der ersten Gleichung erhalten wir\n\n\\begin{align*}\n\\sum_{i=1}^n y_i - \\hat{\\beta}_0 n - \\hat{\\beta}_1 \\sum_{i=1}^n x_i & = 0 \\\\\n\\hat{\\beta}_0 n + \\hat{\\beta}_1 \\sum_{i=1}^n x_i & = \\sum_{i=1}^n y_i \\\\\n\\hat{\\beta}_0 & = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\n\\end{align*}\n\nDie zweite Gleichung ergibt\n\n\\begin{align*}\n\\sum_{i=1}^n y_i x_i - \\hat{\\beta}_0 \\sum_{i=1}^n x_i - \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = 0 \\\\\n\\hat{\\beta}_0 \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i \\\\\n\\hat{\\beta}_0 & = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\n\\end{align*}\n\nWir setzen für \\hat{\\beta}_0 in die zweite Gleichung ein und erhalten\n\n\\begin{align*}\n(\\overline{y} - \\hat{\\beta}_1 \\overline{x}) \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i \\\\\n\\overline{y} \\sum_{i=1}^n x_i - \\hat{\\beta}_1 \\overline{x} \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i \\\\\n\\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i - \\overline{y} \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\overline{x} \\sum_{i=1}^n x_i \\\\\n\\hat{\\beta}_1 & = \\frac{\\sum_{i=1}^n y_i x_i - \\overline{y} \\sum_{i=1}^n x_i}{\\sum_{i=1}^n x_i^2 - \\overline{x} \\sum_{i=1}^n x_i}\n\\end{align*}\n\nNun können wir die Ausdrücke vereinfachen und erhalten\n\n\\begin{align*}\n\\hat{\\beta}_1 & = \\frac{\\overline{x y} - \\overline{x} \\cdot \\overline{y}}{\\overline{x^2} - \\overline{x}^2} \\\\\n\\hat{\\beta}_0 & = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\n\\end{align*}\n\nThe last expression may seem a bit complicated, but it is actually quite simple. It is just the ratio between the empirical covariance between x_i and y_i divided by the variance of x_i.\nThe empirical covariance between x_i and y_i is defined as the sum of the products of the deviations of x_i and y_i from their respective means, divided by the number of observations.\n\nDefinition 7.1 (Empirische Kovarianz) Die empirische Kovarianz zwischen zwei Variablen x und y mit n Werten ist definiert als\n\nS_{xy} = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})\n\n\n\nTheorem 7.1 (Alternative Formel für die Kovarianz) Die Kovarianz definiert in Definition B.3 lässt sich auch darstellen als\n\n(n - 1) S_{xy} = n(\\overline{x y} - \\overline{x} \\overline{y})\n\n\n\n\n\n\n\n\nBeweis\n\n\n\n\n\nDer Beweis folgt den gleichen Schritten wie der Beweis für die alternative Darstellung der empirischen Varianz.\n\n\\begin{align*}\n(n - 1) S_{xy} & = \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) \\\\\n& = \\sum_{i=1}^n x_i y_i - \\overline{x} \\sum_{i=1}^n y_i - \\overline{y} \\sum_{i=1}^n x_i + n \\overline{x} \\overline{y} \\\\\n& = n(\\overline{x y} - \\overline{x} \\overline{y})\n\\end{align*}\n\n\n\n\n\nTheorem 7.2 (Alternative Varianzformel) Wir haben bereits die empirische Kovarianz definiert als\n\nS_x^2 = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\overline{x})^2\n\nWir können zeigen, dass die folgende Darstellung für die empirische Varianz gilt\n\nS_x^2 = \\frac{n}{n  - 1}(\\overline{x_i^2} - \\overline{x}^2)\n\n\n\n\n\n\n\n\nBeweis\n\n\n\n\n\nWir benutzen die Definition der Varianz und die Definition des Mittelwertes.\n\n\\begin{align*}\n(n - 1) S_x^2 & =  \\sum_{i=1}^n (x_i - \\overline{x})^2 \\\\\n& =  \\sum_{i=1}^n (x_i^2 - 2x_i \\overline{x} + \\overline{x}^2) \\\\\n& =  \\sum_{i=1}^n x_i^2 - 2\\overline{x} \\sum_{i=1}^n x_i + \\overline{x}^2 \\sum_{i=1}^n 1 \\\\\n& =  \\sum_{i=1}^n x_i^2 - 2\\overline{x} \\sum_{i=1}^n x_i + \\overline{x}^2 n \\\\\n& =  \\sum_{i=1}^n x_i^2 - 2\\overline{x}^2 n + \\overline{x}^2 n \\\\\n& =  \\sum_{i=1}^n x_i^2 - n \\overline{x}^2 \\\\\n& = n (\\overline{x^2} - \\overline{x}^2)\n\\end{align*}\n\n\n\n\n\n# Set seed for reproducibility\nnp.random.seed(0)\n\n# Create a DataFrame\nx = np.random.normal(size=100, loc=0, scale=1)\ny = 2 * x + np.random.normal(size=100, scale=1)\n\nprod_pos = (x &gt; x.mean()) * (y &gt; y.mean()) + (x &lt;= x.mean()) * (y &lt;= y.mean())\nprod_neg = np.logical_not(prod_pos)\n\n# Plot\nplt.scatter(x[prod_pos], y[prod_pos], color=\"black\")\nplt.scatter(x[prod_neg], y[prod_neg], color=\"red\")\n\nplt.axvline(x.mean(), color='firebrick', label='Mittelwert von x')\nplt.axhline(y.mean(), color='steelblue', label='Mittelwert von y')\n\n\n\n\n\n\n\nAbbildung 7.6: Streudiagramm mit positiver Kovarianz\n\n\n\n\n\nDie rote Linie ist bei dem Durchschnitt der x-Werte gezeichnet und die blaue Linie ist bei dem Durchschnitt der y-Werte gezeichnet. Die Kovarianz misst das durchschnittliche Produkt der Abweichungen der x-Werte von ihrem Durchschnitt und der y-Werte von ihrem Durchschnitt. Das Produkt ist positiv, wenn die x-Werte und die y-Werte gleichzeitig über oder unter ihren Durchschnitten liegen. Das Produkt ist negativ, wenn die x-Werte über ihrem Durchschnitt liegen, während die y-Werte unter ihrem Durchschnitt liegen, oder umgekehrt.\n\n# Kovarianz von zwei NumPy-Arrays\n# Achten Sie darauf, dass eine ganze Kovarianzmatrix zurückgegeben wird.\n# Die Diagonalelemente sind die Varianzen der einzelnen Arrays.\n\nnp.cov(x, y)\n\narray([[1.02608749, 2.16986562],\n       [2.16986562, 5.65646178]])\n\n\n\nnp.var(x, ddof=1)\n\n1.0260874941564961\n\n\n\nnp.var(y, ddof=1)\n\n5.656461778144082\n\n\nNur das Vorzeichen der Kovarianz ist wichtig. Die Größe der Kovarianz hängt von den Einheiten der Variablen ab. Um die Kovarianz dimensionslos zu machen, können wir sie durch das Produkt der Standardabweichungen der beiden Variablen teilen. Das gibt uns den Korrelationskoeffizienten.\n\nnp.cov(x, 20 * y)\n\narray([[1.02608749e+00, 4.33973123e+01],\n       [4.33973123e+01, 2.26258471e+03]])\n\n\nLasst uns auch ein Beispiel betrachten, wo die Kovarianz negativ ist. In dem folgenden Code ändern wir nur das Vorzeichen des Koeffizienten von x in der Gleichung von y.\n\n\nCode\n# Set seed for reproducibility\nnp.random.seed(23)\n\n# Create a DataFrame\nx1 = np.random.normal(size=100, loc=0, scale=1)\ny1 = - 2 * x1 + np.random.normal(size=100, scale=1)\n\n# Plot\nplt.scatter(x1, y1, color=\"black\")\nplt.axvline(x1.mean(), color='firebrick', label='Mittelwert von x')\nplt.axhline(y1.mean(), color='steelblue', label='Mittelwert von y')\n\n\n\n\n\n\n\n\nAbbildung 7.7: Streudiagramm von x und y mit negativer Kovarianz.\n\n\n\n\n\n\nnp.cov(x1, y1)\n\narray([[ 0.90312011, -1.81047309],\n       [-1.81047309,  4.67610569]])\n\n\n\nx_rand = np.random.uniform(size=100)\ny_rand = np.random.uniform(size=100)\n\nfig, ax = plt.subplots()\nax.scatter(x_rand, y_rand, color=\"black\")\n\nax.axvline(x_rand.mean(), color='firebrick', label='Mittelwert von x')\nax.axhline(y_rand.mean(), color='steelblue', label='Mittelwert von y')\n\n\n\n\n\n\n\nAbbildung 7.8: Streudiagramm von x und y mit unabhängigen Variablen.\n\n\n\n\n\n\nnp.cov(x_rand, y_rand)\n\narray([[ 0.08527383, -0.01966643],\n       [-0.01966643,  0.07947542]])\n\n\n\nnp.corrcoef(x_rand, y_rand)\n\narray([[ 1.        , -0.23889178],\n       [-0.23889178,  1.        ]])\n\n\n\nDefinition 7.2 (Korrelationskoeffizient) Der empirische Korrelationskoeffizient zwischen zwei Variablen x und y mit n Werten ist definiert als\n\nr_{xy} = \\frac{S_{xy}}{S_x S_y}\n\ndabei ist S_{xy} die Kovarianz zwischen x und y, und S_x und S_y sind die Standardabweichungen von x und y.\n\nWeil die Kovarianz durch das Produkt der Standardabweichungen geteilt wird, ist der Korrelationskoeffizient dimensionslos. Außerdem ist der Korrelationskoeffizient immer zwischen -1 und 1. Ein Korrelationskoeffizient von 1 bedeutet, dass die beiden Variablen auf einer Geraden mit positiver Steigung liegen. Ein Korrelationskoeffizient von -1 bedeutet, dass die beiden Variablen auf einer Geraden mit negativer Steigung liegen. Ein Korrelationskoeffizient von 0 bedeutet, dass es keine lineare Assoziation zwischen den beiden Variablen gibt.\nZusammengefasst:\n\n-1 \\leq r_{xy} \\leq 1\n\n\ny = a + b x \\implies r_{xy} = 1, \\quad b &gt; 0\n\n\ny = a - b x \\implies r_{xy} = -1, \\quad b &gt; 0\n\n\n\n\n\n\n\nWieso ist der Korrelationskoeffizient immer zwischen -1 und 1?\n\n\n\n\n\nFür den Beweis werden wir zuerst zeigen, dass sich die Skalierungsfaktoren 1 / (n - 1) im Zähler und im Nenner der Definition des Korrelationskoeffizienten kürzen.\n\n\\begin{align*}\nr_{xy} & = \\frac{S_{xy}}{S_x S_y} \\\\\n    & = \\frac{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\overline{x})^2} \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (y_i - \\overline{y})^2}} \\\\\n    & = \\frac{\\frac{1}{n - 1}}{\\frac{1}{n - 1}}\\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\overline{y})^2}} \\\\\n    & = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\overline{y})^2}}\n\\end{align*}\n\nFür den Beweis brauchen wir zuerst die Cauchy-Schwarz-Ungleichung. Diese Ungleichung besagt, dass das Quadrat des Skalarprodukts zweier Vektoren kleiner oder gleich dem Produkt der Längen der beiden Vektoren ist. Für einen Beweis der Cauchy-Schwarz-Ungleichung siehe Theorem C.1.\nBetrachten wir die Vektoren x und y als Vektoren im n-dimensionalen Raum. Der Korrelationskoeffizient ist definiert als\n\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\quad y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}\n\nEs ist einfacher zu rechnen, wenn die Vektoren zentriert sind. Das bedeutet, dass wir die Mittelwerte von x und y von den Vektoren abziehen.\n\nx^{*} = x - \\overline{x} = \\begin{pmatrix} x_1 - \\overline{x} \\\\ x_2 - \\overline{x} \\\\ \\vdots \\\\ x_n - \\overline{x} \\end{pmatrix}, \\quad y^{*}  = y - \\overline{y}= \\begin{pmatrix} y_1 - \\overline{y} \\\\ y_2 - \\overline{y} \\\\ \\vdots \\\\ y_n - \\overline{y} \\end{pmatrix}\n\nDas Skalarprodukt der zentrierten Vektoren ist dann einfach die Summe im Zähler der Definition des Korrelationskoeffizienten.\n\n\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) = x^{*T} y^{*}\n\nNun schauen wir uns den Nenner an. Der Nenner ist das Produkt der Längen der zentrierten Vektoren.\n\n\\sqrt{\\sum_{i=1}^n (x_i - \\overline{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\overline{y})^2} = ||x^{*}|| \\cdot ||y^{*}||\n\n\n\n\n\nnp.corrcoef(x, y)\n\narray([[1.        , 0.90067522],\n       [0.90067522, 1.        ]])\n\n\n\nnp.corrcoef(x1, y1)\n\narray([[ 1.        , -0.88100254],\n       [-0.88100254,  1.        ]])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Die KQ-Methode</span>"
    ]
  },
  {
    "objectID": "03-KQ-Methode.html#übung",
    "href": "03-KQ-Methode.html#übung",
    "title": "7  Die KQ-Methode",
    "section": "7.5 Übung",
    "text": "7.5 Übung\nDefinieren Sie eine Funktion, die für zwei Numpy-Arrays die KQ-Koeffizienten berechnet und als eine Tupel zurückgibt.\n\nx_test = np.random.normal(size=100, loc=0, scale=1)\ny_test = 1 + 2 * x + np.random.normal(size=100, scale=1.5)\n\n\ndef ols_two_variable(x, y):\n    pass\n\n\nfrom statsmodels.formula.api import ols\n\nols(\"y ~ x\", data=pd.DataFrame({'x': x_test, 'y': y_test})).fit().params\n\nIntercept    0.762981\nx           -0.224131\ndtype: float64\n\n\n\n# Bevor wir die Funktion schreiben, versuchen wir die KQ Methode anhand \n# eines einfachen Beispiels berechnen.\n\nx1 = np.array([1, 2, 3, 4, 5])\ny1 = np.array([2, 3, 4, 5, 6])\n\n# Berechnen Sie die Koeffizienten beta_0 und beta_1\n\n# Schritt 1: Berechnen Sie die Mittelwerte von x und y\nx1_mean = x1.mean()\ny1_mean = y1.mean()\n\n# Schritt 2: Berechnen Sie den Mittelwert der quadrierten Werte von x\n\nx1_squared_mean = (x1 ** 2).mean()\n\n# Schritt 3: Berechnen Sie die Mittelwerte der Produkte von x und y\n\nxy_mean = (x1 * y1).mean()\n\n# Schritt 4: Berechnen Sie beta_1_hat\n\nbeta_1_hat = (xy_mean - x1_mean * y1_mean) / (x1_squared_mean - x1_mean ** 2)\n\n# Schritt 5: Berechnen Sie beta_0_hat\n\nbeta_0_hat = y1_mean - beta_1_hat * x1_mean\n\n\n# Wir können unsere Ergebnisse mit der statsmodels-Bibliothek überprüfen\n\nols(\"y ~ x\", data=pd.DataFrame({'x': x1, 'y': y1})).fit().params\n\nIntercept    1.0\nx            1.0\ndtype: float64",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Die KQ-Methode</span>"
    ]
  },
  {
    "objectID": "03-KQ-Methode.html#anwendung-der-kq-methode",
    "href": "03-KQ-Methode.html#anwendung-der-kq-methode",
    "title": "7  Die KQ-Methode",
    "section": "7.6 Anwendung der KQ-Methode",
    "text": "7.6 Anwendung der KQ-Methode\nNachdem wir eine Idee darüber gewonnen haben, wie die KQ-Methode die optimalen Koeffizienten in der Prognosegleichung bestimmt, lassen Sie und die KQ-Methode auf die Daten des Buchhaltungsunternehmens anwenden.\nDas Modell, das wir verwenden, ist\n\n\\widehat{\\text{Time}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\text{Invoices}\n\n\nmodel = ols(\"Time ~ Invoices\", data=invoices).fit()\nmodel.params\n\nIntercept    0.641710\nInvoices     0.011292\ndtype: float64\n\n\nInterpretation der Koeffizienten:\n\nMaßeinheit von \\hat{\\beta}_0: ?\nMaßeinheit von \\hat{\\beta}_1: ?\nWirtschaftliche Interpretation von \\hat{\\beta}_0: ?\nWirtschaftliche Interpretation von \\hat{\\beta}_1: ?\n\nVisualisierung der Prognosen:\n\n# Zuerst berechnen wir die Prognosewerte für jeden Tag\n\n# Wir könnten die Prognosewerte \"per Hand\" ausrechnen, wie wir es oben gemacht haben.\n# Der einzige Unterschied ist es, dass wir die Koeffizienten aus dem Modell nehmen.\n\nTime_predicted_OLS_hand1 = 0.641710 + 0.011292 * invoices['Invoices']\n\n# Oder wir können die Koeffizienten aus dem Modellobjekt herauslesen\n\nTime_predicted_OLS_hand2 = model.params['Intercept'] + model.params['Invoices'] * invoices['Invoices']\n\n# Wir werden allerdings die .predict()-Methode des Modells verwenden, da es automatisch die Prognosewerte berechnet.\n\nTime_predicted_OLS = model.predict()\n\nDie Prognosewerte können wir nun graphisch darstellen\n\nfig, ax = plt.subplots()\n\nsns.scatterplot(data=invoices, x=\"Invoices\", y=\"Time\", ax=ax)\nax.plot(invoices[\"Invoices\"], Time_predicted_OLS, \"-\", color='firebrick', label=r\"$\\widehat{Time} = 0.6417 + 0.0113 \\cdot \\text{Invoices}$\")\n\nax.legend(loc=0)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Die KQ-Methode</span>"
    ]
  },
  {
    "objectID": "03-KQ-Methode.html#die-geometrie-der-kq-methode",
    "href": "03-KQ-Methode.html#die-geometrie-der-kq-methode",
    "title": "7  Die KQ-Methode",
    "section": "7.7 Die Geometrie der KQ-Methode",
    "text": "7.7 Die Geometrie der KQ-Methode\nUm die Darstellung einfach zu halten, betrachten wir einen Datensatz mit nur zwei Beobachtungen und eine Prognosegleichung ohne Achsenabschnitt (Intercept).\n\n\\hat{y} = \\hat{\\beta}_1 x\n\n\ndt_2d = pd.DataFrame({'x': [1, 1.5], 'y': [1, 0.2]})\ndt_2d\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n1.0\n1.0\n\n\n1\n1.5\n0.2\n\n\n\n\n\n\n\n\nDie Daten (y-Werte) können wir als einen zweidimensionalen Vektor betrachten (eine Dimension pro jede Beobachtung). Die Prädiktorvariable x können wir auch als einen zweidimensionalen Vektor auffassen.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = np.array([2, 3]) / 2\nY = np.array([1, 0.2])\n\n# Create a figure\nfig, ax = plt.subplots()\nY_proj = np.dot(X, Y) / np.dot(X, X) * X\nY_min_Y_proj = Y_proj - Y\n\n# Plot the vectors\nax.quiver(0, 0, X[0], X[1], angles='xy', scale_units='xy', scale=1, color='r', label='A')\nax.quiver(0, 0, Y[0], Y[1], angles='xy', scale_units='xy', scale=1, color='b', label='B')\nax.quiver(Y[0], Y[1], Y_min_Y_proj[0], Y_min_Y_proj[1], angles='xy', scale_units='xy', scale=1, color='c', label='C')\nax.quiver(0, 0, Y_proj[0], Y_proj[1], angles='xy', scale_units='xy', scale=1, color='g', label='D')\nax.set_xlim(0, 1.5)\nax.set_ylim(0, 1.5)\n\nax.annotate('x', (X[0], X[1]), textcoords=\"offset points\", xytext=(2,-10), ha='center')\nax.annotate('y', (Y[0], Y[1]), textcoords=\"offset points\", xytext=(2,-10), ha='center')\nax.annotate(r\"$ \\hat{y} = \\hat{\\beta}_1 x$\", (Y_proj[0], Y_proj[1]), textcoords=\"offset points\", xytext=(-20,0), ha='center')\nax.annotate(r\"$\\hat{\\beta}_1 x - y$\", (Y_proj[0], Y_proj[1]), textcoords=\"offset points\", xytext=(70,-20), ha='center')\n\n\n\n\n\n\n\nText(70, -20, '$\\\\hat{\\\\beta}_1 x - y$')\n\n\n(a) Projektion des Vektors y auf den Vektor x.\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nAbbildung 7.9\n\n\n\n\nIm zweidimensionalen können wir die Variable x als einen Vector mit Koordinaten (x_1, x_2) darstellen. Die Variable y können wir als einen Vektor mit Koordinaten (y_1, y_2) darstellen. Unsere Aufgabe ist nun, den Vektor \\hat{y} = \\hat{\\beta}_1 x so zu wählen, dass der Abstand zwischen den Vektoren y und \\hat{y} minimal ist.\nDer Abstand zwischen zwei Vektoren a und b ist definiert als die Länge des Vektors a - b. Den kleinsten Abstand erhalten wir, wenn die Zahl \\hat{\\beta}_1 so gewählt wird, dass der Vektor y - \\hat{y} senkrecht auf dem Vektor x steht. Das bedeutet, dass das Skalarprodukt von y - \\hat{y} und x gleich Null sein muss.\n\nx \\cdot (y - \\hat{y}) = 0\n\nDas Skalarprodukt von zwei Vektoren a = (a_1, a_2) und b = (b_1, b_2) ist definiert als\n\na \\cdot b = a_1 b_1 + a_2 b_2\n\noder für a = (a_1, a_2, \\ldots, a_n) und b = (b_1, b_2, \\ldots, b_n)\n\na \\cdot b = \\sum_{i=1}^n a_i b_i = a_1 b_1 + a_2 b_2 + \\ldots + a_n b_n\n\nWir können das Skalarprodukt auch als Matrixmultiplikation schreiben\n\na \\cdot b = a^T b = \\begin{pmatrix} a_1 & a_2 & \\ldots & a_n \\end{pmatrix} \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{pmatrix} = a_1 b_1 + a_2 b_2 + \\ldots + a_n b_n\n\nAus der Bedingung x \\cdot (y - \\hat{y}) = 0 erhalten wir\n\n\\begin{align*}\nx^T (y - \\hat{y}) & = x^T y - x^T \\hat{y} = 0 \\\\\nx^T y & = x^T \\hat{y} \\\\\nx^T y & = x^T \\hat{\\beta}_1 x \\\\\nx^T y & = \\hat{\\beta}_1 x^T x \\\\\n\\hat{\\beta}_1 & = \\frac{x^T y}{x^T x}\n\\end{align*}\n\nIm letzten Schritt dürfen wir durch x^T x teilen, weil x^T x eine Zahl (nicht ein Vektor oder eine Matrix) und ungleich Null ist (es ist die Summe der Quadrate der Elemente von x).\nDieser Ansatz lässt sich auf mehrdimensionale Daten verallgemeinern. Die Prognosegleichung lautet dann (für p Prädiktorvariablen und n Beobachtungen)\n\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\ldots + \\hat{\\beta}_p x_p\n\nDie einzelnen Gleichungen für jede Beobachtung sind\n\n\\begin{align*}\n\\hat{y}_1 & = \\hat{\\beta}_1 x_{11} + \\hat{\\beta}_2 x_{12} + \\ldots + \\hat{\\beta}_p x_{1p} \\\\\n\\hat{y}_2 & = \\hat{\\beta}_1 x_{21} + \\hat{\\beta}_2 x_{22} + \\ldots + \\hat{\\beta}_p x_{2p} \\\\\n\\vdots & \\\\\n\\hat{y}_n & = \\hat{\\beta}_1 x_{n1} + \\hat{\\beta}_2 x_{n2} + \\ldots + \\hat{\\beta}_p x_{np}\n\\end{align*}\n\nDieses Gleichungssystem lässt sich in Matrixschreibweise darstellen\n\n\\begin{pmatrix}\n\\hat{y}_1 \\\\\n\\hat{y}_2 \\\\\n\\vdots \\\\\n\\hat{y}_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\nx_{21} & x_{22} & \\ldots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\ldots & x_{np}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\hat{\\beta}_1 \\\\\n\\hat{\\beta}_2 \\\\\n\\vdots \\\\\n\\hat{\\beta}_p\n\\end{pmatrix}\n\noder kurz\n\n\\hat{y}_{1 \\times n} = X_{n \\times p} \\hat{\\beta}_{p \\times 1}\n\nEine ähnliche Herleitung wie oben wird verlangen, daß der Prognosevektor orthogonal auf den Spalten von X steht. Das bedeutet, daß die Matrix X^T (y - \\hat{y}) gleich Null sein muss.\n\n\\begin{align*}\nX^T (y - \\hat{y}) & = 0 \\\\\nX^T y & = X^T \\hat{y} \\\\\nX^T y & = X^T X \\hat{\\beta} \\\\\n\\end{align*}\n\nHier können wir allerdings nicht einfach durch X^T X dividieren, als wir es im zweidimensionalen Fall gemacht haben, da X^T X eine ganze Matrix ist. Wir können allerdings die Inverse der Matrix X^T X verwenden. Diese bezeichnen wir mit (X^T X)^{-1}.\nDie Inverse einer quadratischen Matrix ist eine Matrix, die, wenn sie mit der ursprünglichen Matrix multipliziert wird, die Einheitsmatrix ergibt.\n\nA A^{-1} = A^{-1} A = I\n\n\nA = np.array([[1, 2], [-2, 1]])\nA\n\narray([[ 1,  2],\n       [-2,  1]])\n\n\n\n# Mit np.linalg.inv() berechnen wir die Inverse einer Matrix\n\nA_inverse = np.linalg.inv(A)\nA_inverse\n\narray([[ 0.2, -0.4],\n       [ 0.4,  0.2]])\n\n\n\n# The @ operator is used for matrix multiplication this is different from A * A_inverse\n# A * A_inverse\n\n# Die Multiplikation von A mit der Inversen von A ergibt die Einheitsmatrix (1 auf der Diagonale, 0 sonst)\nA @ A_inverse\n\narray([[1., 0.],\n       [0., 1.]])\n\n\n\nA_inverse @ A\n\narray([[1., 0.],\n       [0., 1.]])\n\n\nFalls die Inverse von X^T X existiert, können wir die Koeffizienten \\hat{\\beta} wie folgt darstellen\n\n\\begin{align*}\n\\underbrace{(X^T X)^{-1} (X^T X)}_{I} \\hat{\\beta} = (X^T X)^{-1} X^T y \\\\\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n\\end{align*}\n\nMan kann zeigen, daß die Inverse von X^T X dann existiert, wenn die Spalten von X linear unabhängig sind. Das bedeutet, daß keine Spalte durch eine Linearkombination der anderen Spalten dargestellt werden kann.\n\nX1 = np.array([[1, 2], [1, 4], [1, 2], [1, -2]])\nprint(X1)\n\nX1.T @ X1\n\n[[ 1  2]\n [ 1  4]\n [ 1  2]\n [ 1 -2]]\n\n\narray([[ 4,  6],\n       [ 6, 28]])\n\n\n\nnp.linalg.inv(X1.T @ X1)\n\narray([[ 0.36842105, -0.07894737],\n       [-0.07894737,  0.05263158]])\n\n\n\n# Schauen wir uns was passiert, wenn wir die zweite Spalte auf 2 (z.B) setzen\n\nX2 = X1.copy()\n\nX2[:, 1] = 2\nX2\n\narray([[1, 2],\n       [1, 2],\n       [1, 2],\n       [1, 2]])\n\n\n\nX2.T @ X2\n\narray([[ 4,  8],\n       [ 8, 16]])\n\n\n\n# np.linalg.inv(X2.T @ X2)\n\nDasselbe passiert, falls eine Spalte von X eine Linearkombination der anderen Spalten ist.\n\nX3 = X1.copy()\nnew_column = X3[:,0] + X3[:,1]\nprint(new_column)\n\nX3_new = np.concatenate([X3, new_column.reshape(4, 1)], axis=1)\nX3_new\n\n[ 3  5  3 -1]\n\n\narray([[ 1,  2,  3],\n       [ 1,  4,  5],\n       [ 1,  2,  3],\n       [ 1, -2, -1]])\n\n\n\n\nX3_new.T @ X3_new\n\narray([[ 4,  6, 10],\n       [ 6, 28, 34],\n       [10, 34, 44]])\n\n\n\n# np.linalg.inv(X3_new.T @ X3_new)\n\n\n# np.linalg.det(X3_new.T @ X3_new)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Die KQ-Methode</span>"
    ]
  },
  {
    "objectID": "04a-Diskrete-Verteilungen.html",
    "href": "04a-Diskrete-Verteilungen.html",
    "title": "8  Diskrete Verteilungen",
    "section": "",
    "text": "8.1 Sampling from a univariate distribution\nIm Folgenden werden wir die Funktion choice aus der NumPy-Bibliothek verwenden, um eine Stichprobe von 10,000 Beobachtungen aus der Verteilung von X zu ziehen.\n# Das erste Argument sind die möglichen Werte\n# Das zweite Argument (p) sind die Wahrscheinlichkeiten\n# Das dritte Argument (size) ist die Anzahl der Zufallszahlen\n\nsmpl_x = np.random.choice(px[\"x\"], p = px[\"p\"], size = 10000)\n\n# Wir können uns die ersten 10 Zufallszahlen anschauen\nsmpl_x[0:10]\n\narray([3, 2, 2, 3, 2, 2, 2, 2, 0, 2])\nWir können die absoluten Häufigkeiten (Anzahl) der Werte zählen\npd.value_counts(smpl_x)\n\n/tmp/ipykernel_2342/1849734910.py:1: FutureWarning:\n\npandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n\n\n\n3    3895\n2    2710\n0    2471\n1     924\nName: count, dtype: int64\nund die relative Häufigkeit berechnen, indem wir die Anzahl durch die Gesamtzahl der Beobachtungen teilen.\npd.value_counts(smpl_x, normalize=True)\n\n/tmp/ipykernel_2342/979932776.py:1: FutureWarning:\n\npandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n\n\n\n3    0.3895\n2    0.2710\n0    0.2471\n1    0.0924\nName: proportion, dtype: float64\nsns.countplot(x = smpl_x)\n# Benutzen Sie den Code aus der vorherigen Zelle, um eine Stichprobe von $Y$ zu ziehen. Achten Sie\n# darauf, dass die Spalte mit den möglichen Werten von $Y$ \"y\" heißt.\n# Die absoluten Häufigkeiten\n# Die relativen Häufigkeiten\n# Das Balkendiagramm",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diskrete Verteilungen</span>"
    ]
  },
  {
    "objectID": "04a-Diskrete-Verteilungen.html#sampling-from-a-univariate-distribution",
    "href": "04a-Diskrete-Verteilungen.html#sampling-from-a-univariate-distribution",
    "title": "8  Diskrete Verteilungen",
    "section": "",
    "text": "Übungsaufgabe 8.1 (Sampling from the Distribution of Y) Ziehen Sie eine Stichprobe von 10,000 Beobachtungen aus der Verteilung von Y und berechnen Sie die absoluten und die relativen Häufigkeiten der Werte und visualisieren Sie die Häufigkeiten mit einem Balkendiagramm.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diskrete Verteilungen</span>"
    ]
  },
  {
    "objectID": "04a-Diskrete-Verteilungen.html#der-erwartungswert",
    "href": "04a-Diskrete-Verteilungen.html#der-erwartungswert",
    "title": "8  Diskrete Verteilungen",
    "section": "8.2 Der Erwartungswert",
    "text": "8.2 Der Erwartungswert\nDer Erwartungswert einer Zufallsvariablen (Verteilung) ist der gewichtete Durchschnitt aller möglichen Werte, die auftreten können. Die Gewichte sind die Wahrscheinlichkeiten, mit denen die Werte auftreten. Der Erwartungswert ist ein Maß für die Lage der Verteilung.\n\n\\begin{align}\n\\mu_x & = E(X) = \\sum_{x = 0}^{3} x p_X(x) = 0 \\times 0.250 + 1 \\times 0.095 + 2 \\times 0.272 + 3 \\times 0.383 = 1.788 \\\\\n\\end{align}\n\n\nx_expected_value = np.sum(px[\"x\"] * px[\"p\"])\nx_expected_value\n\n1.788\n\n\n\nnp.mean(smpl_x)\n\n1.8029\n\n\n\nfig, ax = plt.subplots()\n\nsns.barplot(x='x', y='p', data=px, ax = ax, fill = False)\nax.axvline(x = x_expected_value, color = \"firebrick\", linestyle = \"--\")\n\n\n\n\n\n\n\n\n\nÜbungsaufgabe 8.2 (Erwartungswert) Berechnen Sie den Erwartungswert von Y.\n\nDer Erwartungswert ist die beste Vorhersage für zukünftige Werte einer Zufallsvariablen, in dem Sinne, dass er den erwarteten Wert der quadratischen Verlustfunktion minimiert:\n\nE[(X - \\hat{x})^2]\n\nLassen Sie uns ein Beispiel konstruieren. Sie müssen das Ergebnis von X vorhersagen und Sie denken, dass die beste Vorhersage \\bar{x} = 1 ist. Wenn das Spiel läuft, wird es vier mögliche Werte produzieren: 0, 1, 2 und 3. Die Fehler, den Sie machen werden, sind:\n\nL(x) = (x - 1)^2 =\n\\begin{cases}\n   (0 - 1)^2 = 1 & \\text{x = 0}\\\\\n   (1 - 1)^2 = 0 & \\text{x = 1}\\\\\n   (2 - 1)^2 = 1 & \\text{x = 2}\\\\\n   (3 - 1)^2 = 4 & \\text{x = 3}\n\\end{cases}\n\n\nÜbungsaufgabe 8.3 (Erwarteter Verlust) Berechnen Sie den erwarteten Verlust für eine Vorhersage von \\bar{x} = 1.5.\n\n\n# Berechnen Sie zuerst die Fehler\n\n# errors = ... - 1.5\n# errors\n\n\n# Danach quadrieren Sie die Fehler\n## squared_errors = ...**2\n\n\n# Zuletzt multiplizieren Sie die quadrierten Fehler mit den Wahrscheinlichkeiten und summieren Sie\n\n# np.sum(... * ...)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diskrete Verteilungen</span>"
    ]
  },
  {
    "objectID": "04a-Diskrete-Verteilungen.html#die-varianz",
    "href": "04a-Diskrete-Verteilungen.html#die-varianz",
    "title": "8  Diskrete Verteilungen",
    "section": "8.3 Die Varianz",
    "text": "8.3 Die Varianz\nDie Varianz einer Zufallsvariablen (Verteilung) misst, wie unterschiedlich die möglichen Werte sind, die auftreten können. Werte, die unter p_X häufiger auftreten (eine höhere Wahrscheinlichkeit haben), erhalten ein höheres Gewicht. Werte, die unter p_X seltener auftreten, erhalten ein geringeres Gewicht in der Summe.\n\nDefinition 8.1 (Varianz) Die Varianz ist als die erwartete quadratische Abweichung von dem Erwartungswert definiert\n\nVar(X) = E\\left( (X - E(X))^2\\right)\n\nFür diskrete Verteilungen ist diese Erwartung einfach der gewichtete (mit den Wahrscheinlichkeiten) Durchschnitt der quadrierten Abweichungen vom Erwartungswert.\n\nIn unserem Beispiel ist die Varianz von X:\n\nVar(X) = \\sum_{x = 0}^{3} (x - \\mu_x)^2 \\times p_X(x)\n\n\n\\begin{align}\nVar(X) & = \\sum_{x = 0}^{3} (x - \\mu_x)^2 \\times p_X(x) \\\\\n       & = (0 - 1.788)^2 \\times 0.250 + (1 - 1.788)^2 \\times 0.095 + (2 - 1.788)^2\\times 0.272 + (3 - 1.788)^2 \\times 0.383 \\\\\n       & = (-1.788)^2 \\times 0.250 + (-0.788)^2 \\times 0.095 + (0.212)^2\\times 0.272 + (1.212)^2 \\times 0.383 \\\\\n       & = 3.196 \\times 0.250 + 0.620^2 \\times 0.095 + 0.044 \\times 0.272 + 1.468 \\times 0.383 \\\\\n       & \\approx 1.433\n\\end{align}\n\\tag{8.1}\n\nnp.sum(((px[\"x\"] - x_expected_value)**2) * px[\"p\"])\n\n1.433056\n\n\n\nTheorem 8.1 (Eigenschaften des Erwartungswertes) Es sei X eine Zufallsvariable mit Erwartungswert E(X), Y eine Zufallsvariable mit Erwartungswert E(Y) und a eine feste Konstante (a \\in \\mathbb{R}). Man kann zeigen, dass die folgenden Eigenschaften gelten:\n\n\\begin{align}\nE(a) & = a \\\\\nE(aX) & = aE(X) \\\\\nE(X + Y) & = E(X) + E(Y)\n\\end{align}\n\nFerner, wenn X und Y unkorreliert sind, dann ist der Erwartungswert des Produkts der beiden Zufallsvariablen gleich dem Produkt ihrer Erwartungswerte:\n\nE(XY) = E(X)E(Y)\n\n\n\n\n\n\n\n\nProof of the Properties of the Expectation\n\n\n\n\n\n\nE(a) = a\n\nThe expected value of a discrete variable X with possible outcomes x_1, x_2, \\ldots, x_n and probabilities p_1, p_2, \\ldots, p_n is given by\n\nE(X) = \\sum_{i = 1}^{n} x_i p_i\n\nMultiplying both sides by a gives\n\naE(X) = a\\sum_{i = 1}^{n} x_i p_i = \\sum_{i = 1}^{n} ax_i p_i\n\nThe right-hand side is the expected value of a random variable that takes the values ax_1, ax_2, \\ldots, ax_n with probabilities p_1, p_2, \\ldots, p_n. Therefore, E(aX) = aE(X).\n\nE(X + Y) = E(X) + E(Y)\n\nThis proof involves the joint distribution function of X and Y which we will introduce later. The proof is based on the linearity of the expected value operator.\n\n\n\n\nTheorem 8.2 (Eigenschaften der Varianz) Es sei X eine Zufallsvariable mit Erwartungswert E(X), Y eine Zufallsvariable mit Erwartungswert E(Y) und a eine feste Konstante (a \\in \\mathbb{R}). Man kann zeigen, dass die folgenden Eigenschaften gelten:\n\nVar(X) = E(X^2) - E(X)^2\n\n\n\\begin{align}\nVar(a) & = 0 \\\\\nVar(aX) & = a^2 Var(X)\n\\end{align}\n\nFalls X und Y unkorreliert sind, dann ist die Varianz der Summe der beiden Zufallsvariablen gleich der Summe ihrer Varianzen:\n\nVar(X + Y) = Var(X) + Var(Y)\n\n\n\nÜbungsaufgabe 8.4 (Erwartungswert und Varianz) Benutzen Sie die Use the distributions of X and Y from Tabelle 8.1 and Tabelle 8.2 to compute the expected value and the variance of\n\n2X + 3Y + 1.\n\nAssume that X and Y are independent.\n\n\nLösung. \nE(2X + 3Y + 1) = \\\\\nVar(2X + 3Y + 1) =",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diskrete Verteilungen</span>"
    ]
  },
  {
    "objectID": "04a-Diskrete-Verteilungen.html#die-gemeinsame-verteilung",
    "href": "04a-Diskrete-Verteilungen.html#die-gemeinsame-verteilung",
    "title": "8  Diskrete Verteilungen",
    "section": "8.4 Die gemeinsame Verteilung",
    "text": "8.4 Die gemeinsame Verteilung\nDie gemeinsame Wahrscheinlichkeitsmassenfunktion gibt die Wahrscheinlichkeit des gleichzeitigen Auftretens von x und y an. Zum Beispiel können Sie die Frage stellen: Was ist die Wahrscheinlichkeit von x = 2 und y = 3.\nFür zwei diskrete Zufallsvariablen ist es bequem, die PMF als eine Tabelle darzustellen. Der DataFrame pxy enthält eine Verteilung von X und Y.\n\npxy\n\n\n\n\n\n\n\n\n\nx\ny\np\n\n\n\n\n0\n0\n2\n0.241\n\n\n1\n0\n3\n0.009\n\n\n2\n1\n2\n0.089\n\n\n3\n1\n3\n0.006\n\n\n4\n2\n2\n0.229\n\n\n5\n2\n3\n0.043\n\n\n6\n3\n2\n0.201\n\n\n7\n3\n3\n0.182\n\n\n\n\n\n\n\n\nManchmal ist es allerdings einfacher, die gemeinsame Verteilung in einer Matrix zu visualisieren. Die Matrix wird als Kreuztabelle bezeichnet. Die Zeilen entsprechen den möglichen Werten von X und die Spalten den möglichen Werten von Y. Die Zellen enthalten die Wahrscheinlichkeiten.\n\npxy.pivot(index = \"x\", columns = \"y\", values = \"p\")\n\n\n\nTabelle 8.3: Joint distribution of X and Y.\n\n\n\n\n\n\n\n\n\n\ny\n2\n3\n\n\nx\n\n\n\n\n\n\n0\n0.241\n0.009\n\n\n1\n0.089\n0.006\n\n\n2\n0.229\n0.043\n\n\n3\n0.201\n0.182\n\n\n\n\n\n\n\n\n\n\n\n\np_{XY}(x=2, y=3) = 0.043\n\nWie in dem eindimensionalen Fall, müssen sich die Wahrscheinlichkeiten zu eins summieren.\n\n\\sum_{x = 0}^{3}\\sum_{y = 2}^{3} p_{XY}(x, y) = 1\n\n\nnp.sum(pxy[\"p\"])\n\n1.0",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diskrete Verteilungen</span>"
    ]
  },
  {
    "objectID": "04a-Diskrete-Verteilungen.html#randverteilungen",
    "href": "04a-Diskrete-Verteilungen.html#randverteilungen",
    "title": "8  Diskrete Verteilungen",
    "section": "8.5 Randverteilungen",
    "text": "8.5 Randverteilungen\nAus der gemeinsamen PMF können wir die Randverteilungen von X und Y ableiten.\n\np_X(x) = \\sum_{y} p_{XY}(x, y)\n\n\npxy.groupby(\"x\")[\"p\"].sum()\n\nx\n0    0.250\n1    0.095\n2    0.272\n3    0.383\nName: p, dtype: float64\n\n\n\nÜbungsaufgabe 8.5 (Randverteilungen) Berechnen Sie die Randverteilungen von X und Y aus der gemeinsamen Verteilung p_{XY}.\n\n\n# pxy.groupby(...)[...].sum()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diskrete Verteilungen</span>"
    ]
  },
  {
    "objectID": "04a-Diskrete-Verteilungen.html#bedingte-verteilungen",
    "href": "04a-Diskrete-Verteilungen.html#bedingte-verteilungen",
    "title": "8  Diskrete Verteilungen",
    "section": "8.6 Bedingte Verteilungen",
    "text": "8.6 Bedingte Verteilungen\n\n\nCode\npxy[\"p_y_given_x\"] = pxy[\"p\"] / pxy.groupby(\"x\")[\"p\"].transform(\"sum\")\n\npxy.pivot(index = \"x\", columns = \"y\", values = \"p_y_given_x\")\n\n\n\n\nTabelle 8.4: Conditional distributions of Y given X\n\n\n\n\n\n\n\n\n\n\ny\n2\n3\n\n\nx\n\n\n\n\n\n\n0\n0.964000\n0.036000\n\n\n1\n0.936842\n0.063158\n\n\n2\n0.841912\n0.158088\n\n\n3\n0.524804\n0.475196\n\n\n\n\n\n\n\n\n\n\n\nDie bedingte Verteilung von Y gegeben X ist die Wahrscheinlichkeitsverteilung von Y unter der Bedingung, dass X einen bestimmten Wert annimmt. Die bedingte Verteilung wird als p_{Y|X}(y | x) geschrieben.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diskrete Verteilungen</span>"
    ]
  },
  {
    "objectID": "04a-Diskrete-Verteilungen.html#unabhängigkeit",
    "href": "04a-Diskrete-Verteilungen.html#unabhängigkeit",
    "title": "8  Diskrete Verteilungen",
    "section": "8.7 Unabhängigkeit",
    "text": "8.7 Unabhängigkeit\nZwei Zufallsvariablen sind unabhängig, wenn das Ergebnis einer der Variablen die Wahrscheinlichkeitsverteilung der anderen nicht beeinflusst. Stellen Sie sich vor, Sie haben zwei Lottoscheine: einen von einer Lotterie in Deutschland und einen von einer Lotterie in Bulgarien. Es wäre sicher anzunehmen, dass die realisierten Gewinne der deutschen Lotterie die Gewinnchancen des bulgarischen Tickets nicht beeinflussen.\nBetrachten wir nun den Fall von abhängigen Zufallsvariablen. Sei X der Pegel eines Flusses (an einer Messstelle) zur Zeit t und Y der Pegel des gleichen Flusses fünf Minuten später. Es wäre sicher anzunehmen, dass wenn der Pegel des Flusses bei t hoch war, dies die Verteilung des Niveaus des Flusses bei t plus fünf Minuten beeinflussen würde.\n\nDefinition 8.2 (Unabhängigkeit) Zwei Zufallsvariablen X und Y sind unabhängig, wenn die gemeinsame PMF gleich dem Produkt der Randverteilungen ist.\n\np_{XY}(x, y) = p_X(x)p_Y(y)\n\n\n\n8.7.1 Bedingte Verteilungen unter Unabhängigkeit\nWir können die gemeinsame Verteilung p_{XY}(x, y) unter der Annahme der Unabhängigkeit von X und Y konstruieren, indem Wir für jede mögliche Kombination von X und Y das Produkt der Randverteilungen berechnen.\n\np_{XY}(x, y) = p_X(x)p_Y(y)\n\n\n\nCode\npxy['p_x'] = pxy.groupby('x')['p'].transform('sum')\npxy['p_y'] = pxy.groupby('y')['p'].transform('sum')\n\npxy['p_xy_ind'] = pxy['p_x'] * pxy['p_y']\n\npxy.pivot(index = \"x\", columns = \"y\", values = \"p_xy_ind\")\n\n\n\n\n\n\n\n\n\ny\n2\n3\n\n\nx\n\n\n\n\n\n\n0\n0.19000\n0.06000\n\n\n1\n0.07220\n0.02280\n\n\n2\n0.20672\n0.06528\n\n\n3\n0.29108\n0.09192\n\n\n\n\n\n\n\n\nNun schauen wir uns die bedingte Verteilung von Y gegeben X an. Die bedingte Verteilung von Y gegeben X unter der Annahme der Unabhängigkeit ist gleich der Randverteilung von Y.\n\n\nCode\npxy[\"p_y_given_x_ind\"] = pxy[\"p_xy_ind\"] / pxy.groupby(\"x\")[\"p_xy_ind\"].transform(\"sum\")\npxy.pivot(index = \"x\", columns = \"y\", values = \"p_y_given_x_ind\")\n\n\n\n\nTabelle 8.5: Joint distribution of X and Y under independence.\n\n\n\n\n\n\n\n\n\n\ny\n2\n3\n\n\nx\n\n\n\n\n\n\n0\n0.76\n0.24\n\n\n1\n0.76\n0.24\n\n\n2\n0.76\n0.24\n\n\n3\n0.76\n0.24",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diskrete Verteilungen</span>"
    ]
  },
  {
    "objectID": "04a-Diskrete-Verteilungen.html#der-bedingte-erwartungswert",
    "href": "04a-Diskrete-Verteilungen.html#der-bedingte-erwartungswert",
    "title": "8  Diskrete Verteilungen",
    "section": "8.8 Der bedingte Erwartungswert",
    "text": "8.8 Der bedingte Erwartungswert\nWir haben gesehen, wie wir die bedingten Verteilungen von Y gegeben X im vorherigen Abschnitt hergeleitet haben. Nun können wir die Frage stellen: Was ist der Erwartungswert von Y, wenn X bereits den Wert 0 angenommen hat (zum Beispiel). Wir können die bedingte Verteilung von Y gegeben X = 0 nehmen und den Erwartungswert dieser Verteilung berechnen.\nFür die unabhängige gemeinsame Verteilung in Tabelle 8.5 ist der bedingte Erwartungswert von Y gegeben X = 0:\n\nE(Y | X=0) = \\sum_{y = 2}^{3} y p_{Y|X=0}(y) = 2 \\times 0.76 + 3 \\times 0.24 = 2.24\n\nFür die abhängige gemeinsame Verteilung in Tabelle 8.3 ist der bedingte Erwartungswert von Y gegeben X = 0:\n\nE(Y | X=0) = \\sum_{y = 2}^{3} y p_{Y|X=0}(y) = 2 \\times 0.964 + 3 \\times 0.036 = 2.036\n\n2 * 0.964 + 3 * 0.036\nWir können den bedingten Erwartungswert von Y für jeden möglichen Wert von X berechnen.\n\nE(Y | X = x) = \\begin{cases}\n  2.036 & \\text{for } x = 0 \\\\\n  2.060 & \\text{for } x = 1 \\\\\n  2.158 & \\text{for } x = 2 \\\\\n  2.475 & \\text{for } x = 3\n\\end{cases}\n\n\n\nCode\npxy.groupby(\"x\").apply(lambda x: np.sum(x[\"p_y_given_x_ind\"] * x[\"y\"]))\n\n\n/tmp/ipykernel_2342/3384244227.py:1: DeprecationWarning:\n\nDataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n\n\n\n\n\nTabelle 8.6: Bedingte Erwartungswerte von Y für alle mögliche Werte von X unter Unabhängigkeit.\n\n\n\nx\n0    2.24\n1    2.24\n2    2.24\n3    2.24\ndtype: float64\n\n\n\n\n\n\n\nCode\npxy.groupby(\"x\").apply(lambda x: np.sum(x[\"p_y_given_x\"] * x[\"y\"]))\n\n\n/tmp/ipykernel_2342/388503762.py:1: DeprecationWarning:\n\nDataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n\n\n\n\n\nTabelle 8.7: Bedingte Erwartungswerte von Y für alle mögliche Werte von X unter Abhängigkeit.\n\n\n\nx\n0    2.036000\n1    2.063158\n2    2.158088\n3    2.475196\ndtype: float64\n\n\n\n\n\nIm Fall der abhängigen Variablen ist der bedingte Erwartungswert von Y gegeben X nicht konstant, sondern variiert mit X.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diskrete Verteilungen</span>"
    ]
  },
  {
    "objectID": "04b-Stetige-Verteilungen.html",
    "href": "04b-Stetige-Verteilungen.html",
    "title": "9  Die Gleichverteilung auf [-1, 1]",
    "section": "",
    "text": "9.1 Die Normalverteilung\nDie Familie der Normalverteilungen wird durch zwei Parameter definiert: den Erwartungswert \\mu und die Standardabweichung \\sigma. Die Dichte der Normalverteilung ist gegeben durch die Formel:\nf(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\nWeil wir diese Verteilung sehr oft benutzen werden, führen wir eine spezielle Notation für die Normalverteilung ein:\nX \\sim N(\\mu, \\sigma^2)\nIn dieser Notation bedeutet X ist normalverteilt mit Erwartungswert \\mu und Varianz \\sigma^2. Sie brauchen die Formel für die Dichte der Normalverteilung nicht auswendig zu lernen, allerdings müssen Sie wissen, wie die Dichte in Abhängigkeit von den Parametern \\mu und \\sigma aussieht.\n\\begin{align*}\nE(X) & = \\mu \\\\\nVar(X) & = \\sigma^2\n\\end{align*}\n# Define the means and standard deviations\nmeans = [0, 2]\nsds = [0.2, 0.5, 1]\n\n# Create a grid of x values\nx = np.linspace(-3, 5, 200)\n\n# Create a DataFrame with all combinations of means, sds, and x values\ndf = pd.DataFrame([(mean, sd, x_val, stats.norm.pdf(x_val, mean, sd)) \n                   for mean in means for sd in sds for x_val in x], \n                  columns=['mean', 'sd', 'x', 'y'])\n\n# Create labels for mean and sd\ndf['mean'] = 'mu = ' + df['mean'].astype(str)\ndf['sd'] = 'sigma = ' + df['sd'].astype(str)\n\n# Plot\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=df, x='x', y='y', hue='mean', style='sd')\nplt.xlabel('x')\nplt.ylabel('Dichte')\n\n/usr/share/miniconda/envs/econ2024/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/usr/share/miniconda/envs/econ2024/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\nText(0, 0.5, 'Dichte')\nEine Art und Weise, wie eine Normalverteilung entstehen kann, ist die Summe von unabhängigen Zufallsvariablen. Das ist der Inhalt des Zentralen Grenzwertsatzes, der besagt, dass die Summe von unabhängigen Zufallsvariablen, die nicht notwendigerweise normalverteilt sind, für eine große Anzahl von Summanden normalverteilt ist.\nUm den Zentralen Grenzwertsatz zu illustrieren, betrachten wir das folgende Spiel.\nplayers_n = 300\ngames_n = 16\n\n# Create a DataFrame similar to expand_grid in R\nunif_games = pd.DataFrame(\n    np.array(\n        np.meshgrid(\n            np.arange(1, games_n + 1),\n            np.arange(1, players_n + 1)\n        )).T.reshape(-1, 2),\n    columns=['game', 'player']\n)\n\n# Add result column with random uniform values between -1 and 1\nunif_games['result'] = np.random.uniform(-1, 1, size=len(unif_games))\n\n# Add initial values for each player\ninitial_values = pd.DataFrame(\n    {'player': np.arange(1, players_n + 1), 'game': 0, 'result': 0})\nunif_games = pd.concat([unif_games, initial_values])\n\n# Sort values and calculate running total for each player\nunif_games = unif_games.sort_values(['player', 'game'])\nunif_games['running_total'] = unif_games.groupby('player')['result'].cumsum()\n\n# Plotting\nplt.figure(figsize=(10, 6))\nfor player in unif_games['player'].unique():\n    player_data = unif_games[unif_games['player'] == player]\n    plt.plot(player_data['game'], player_data['running_total'],\n             color='skyblue', alpha=0.2)\n\n# First player\nplayer_data = unif_games[unif_games['player'] == 1]\nplt.plot(player_data['game'], player_data['running_total'],\n         color='firebrick', label='Player 1')\n\nplt.axhline(0, color='black')\n\nfor mark in [4, 8, 16]:\n    plt.axvline(x=mark, linestyle='--', color='black')\n\nplt.xlabel('Spiel')\nplt.ylabel('Gesamtgewinn')\nplt.xticks([0, 4, 8, 12, 16])\n\n([&lt;matplotlib.axis.XTick at 0x7f9097b48cd0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7f9097b37250&gt;,\n  &lt;matplotlib.axis.XTick at 0x7f909799b790&gt;,\n  &lt;matplotlib.axis.XTick at 0x7f90977cc710&gt;,\n  &lt;matplotlib.axis.XTick at 0x7f90977ce390&gt;],\n [Text(0, 0, '0'),\n  Text(4, 0, '4'),\n  Text(8, 0, '8'),\n  Text(12, 0, '12'),\n  Text(16, 0, '16')])\ngame_4 = unif_games[unif_games['game'] == 4]\n\nplt.figure(figsize=(8, 6))\nsns.kdeplot(data=game_4, x='running_total', fill=True)\nplt.title('Running total distribution at the 4-th game')\nplt.xlabel('Running total')\n\n/usr/share/miniconda/envs/econ2024/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\nText(0.5, 0, 'Running total')\n# Erstellen Sie die Dichteschätzung für das 8. Spiel und das 16. Spiel.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Die Gleichverteilung auf \\[-1, 1\\]</span>"
    ]
  },
  {
    "objectID": "05-Einweg-Varianzanalyse.html",
    "href": "05-Einweg-Varianzanalyse.html",
    "title": "10  ANOVA",
    "section": "",
    "text": "10.1 Das Modell\n\\begin{align*}\n& i = 1,\\ldots, n = 434 \\text{ observations}\\\\\n& y_i: \\text{IQ score} \\\\\n& \\hat{y}_i: \\text{Predicted IQ score} \\\\\n& x_i \\in \\{0, 1\\}: \\text{status of the mother}\n\\end{align*}\ny_i = \\beta_0 + \\beta_1 x_i + e_i, e_i \\sim N(0, \\sigma^2)\n\\begin{align*}\n& y_i \\sim N(\\mu_i, \\sigma^2), \\quad i = 1,\\ldots,n \\\\\n& \\mu_i = \\beta_0 + \\beta_1 x_i, \\quad x_i \\in \\{0, 1\\}\n\\end{align*}\n\\tag{10.1}\n\\mu_1 = \\beta_0 + \\beta_1 \\cdot 1\n\\tag{10.2}\nFür x = 0\n\\mu_0 = \\beta_0 + \\beta_1 \\cdot 0\n\\tag{10.3}\nDie Differenz der zwei Gleichungen 10.2 und 10.3 ergibt:\n\\begin{align*}\n\\beta_0 & = \\mu_0 \\\\\n\\beta_1 & = \\mu_1 - \\mu_0\n\\end{align*}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Define the range of x\nx = np.linspace(0, 180, 1000)\n\n# Calculate the densities of the normal distributions\ny1 = norm.pdf(x, loc=77, scale=20)\ny2 = norm.pdf(x, loc=100, scale=20)\n\n# Create the plot\nplt.figure(figsize=(8, 6))\nplt.plot(x, y1, color='steelblue', label='Mutter ohne HS')\nplt.plot(x, y2, color='firebrick', label='Mother mit HS')\nplt.axvline(x=77, linestyle='--', alpha=0.5, color='firebrick')\nplt.axvline(x=100, linestyle='--', alpha=0.5, color='steelblue')\nplt.text(90, 0.005, r'$\\beta_1$', verticalalignment='bottom', horizontalalignment='center')\nplt.xlim([0, 180])\nplt.xlabel('IQ score')\nplt.ylabel('Dichte')\nplt.legend(loc='upper right')",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "Literature.html",
    "href": "Literature.html",
    "title": "Empfohlene Literatur",
    "section": "",
    "text": "James u. a. (2023)\n\nDas Lehrbuch ist eine Einführung in die statistische Lerntheorie und behandelt das lineare Regressionsmodell, das der Kern des Kurses ist. Enthalten sind Beispielanwendungen in Python.\n\nMcKinney (2022)\n\nDas Buch ist eine Einführung in die Datenanalyse mit Python und behandelt die Grundlagen der Programmiersprache und der Datenanalyse. Es ist ein guter Einstieg in die Programmierung mit Python.\n\nGelman, Hill, und Vehtari (2021):\n\nRegression and other stories. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press (Analytical methods for social research). Available at: https://doi.org/10.1017/9781139161879.\n\n\n\n\nGelman, Andrew, Jennifer Hill, und Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, und Jonathan Taylor. 2023. An Introduction to Statistical Learning with Applications in Python. Cham: Springer International Publishing.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and Jupyter. Third edition. Beijing Boston Farnham Sebastopol Tokyo: O’Reilly.",
    "crumbs": [
      "Empfohlene Literatur"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literaturverzeichnis",
    "section": "",
    "text": "Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and\nOther Stories. Analytical Methods for Social Research.\nCambridge New York, NY Port Melbourne, VIC New Delhi\nSingapore: Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, and\nJonathan Taylor. 2023. An Introduction to\nStatistical Learning with Applications in\nPython. Cham: Springer\nInternational Publishing.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis: Data Wrangling with\nPandas, NumPy, and Jupyter.\nThird edition. Beijing Boston Farnham Sebastopol Tokyo:\nO’Reilly.",
    "crumbs": [
      "Literaturverzeichnis"
    ]
  },
  {
    "objectID": "Appendices/02-Random-Variables.html",
    "href": "Appendices/02-Random-Variables.html",
    "title": "Anhang A — Diskrete Zufallsvariablen",
    "section": "",
    "text": "A.1 Simulation eines Zufallsspiels\nWir schauen uns zwei Spiele an. Im Spiel X können wir nichts (0 EUR), 1 EUR oder 2 EUR gewinnen. Im Spiel Y können wir nichts (0 EUR), 3 EUR oder 4 EUR gewinnen. Jedes Mal, das wir eins der Spiele spielen, wird ein Ergebnis eintreten (wir erhalten 0, 1, oder 2 EUR). Es ist wichtig zu beachten, dass wir das Ergebnis des Spieles vor dem eigentlichen Spielen nicht kennen.\nIm Folgenden werden wir die Spiele X und Y mittels Zufallszahlen simulieren.\nnp.random.seed(432)\n\n# Wir spielen 1000 Mal das Spiel X\nx = np.random.choice([0, 1, 2], size=1000, p = [0.5, 0.3, 0.2])\nx.shape\n\n(1000,)\nnp.random.seed(434)\n\n# Wir Spielen 1000 Mal das Spiel Y und speichern die Ergebnisse\n# in y\ny = np.random.choice([0, 1, 2], size=1000, p=[0.5, 0.1, 0.4])\ny.shape\n\n(1000,)\n# Nun lasst uns die Spielergebnisse zählen\nx_freqs = np.unique(x, return_counts=True)\nx_freqs\n\n(array([0, 1, 2]), array([492, 289, 219]))\ny_freqs = np.unique(y, return_counts=True)\ny_freqs\n\n(array([0, 1, 2]), array([509,  73, 418]))\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n\nsns.barplot(x=x_freqs[0], y = x_freqs[1], ax=ax1)\nsns.barplot(x=y_freqs[0], y=y_freqs[1], ax=ax2)\n\nax1.set_title('Spiel X')\nax2.set_title('Spiel Y')\n\nText(0.5, 1.0, 'Spiel Y')\ngames = pd.DataFrame({\n    'x': [0, 1, 2],\n    \"p_x\": [0.5, 0.3, 0.2],\n    'y': [0, 1, 2],\n    \"p_y\": [0.5, 0.1, 0.4]\n})\n\ngames\n\n\n\nTabelle A.1: Outcomes and probabilities\n\n\n\n\n\n\n\n\n\n\n\nx\np_x\ny\np_y\n\n\n\n\n0\n0\n0.5\n0\n0.5\n\n\n1\n1\n0.3\n1\n0.1\n\n\n2\n2\n0.2\n2\n0.4\nWir können die Wahrscheinlichkeiten der Gewinne in beiden Spielen in einem Balkendiagramm darstellen. Vergleichen Sie die Ergebnisse aus den Simulationen mit then Wahrscheinlichkeiten.\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\nsns.barplot(data=games, x=\"x\", y=\"p_x\", color='blue', ax = ax[0])\nax[0].set_title('Wahrscheinlichkeiten von Spiel X')\nax[0].set_xlabel('x')\nax[0].set_ylabel('Wahrscheinlichkeit')\n\nsns.barplot(data=games, x=\"y\", y=\"p_y\", color='red', ax = ax[1])\nax[1].set_title('Wahrscheinlichkeiten von Spiel Y')\nax[1].set_xlabel('y')\nax[1].set_ylabel('Wahrscheinlichkeit')\n\nText(0, 0.5, 'Wahrscheinlichkeit')",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Diskrete Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "Appendices/02-Random-Variables.html#der-erwartungswert",
    "href": "Appendices/02-Random-Variables.html#der-erwartungswert",
    "title": "Anhang A — Diskrete Zufallsvariablen",
    "section": "A.2 Der Erwartungswert",
    "text": "A.2 Der Erwartungswert\nDie Wahrscheinlichkeiten in der Tabelle oben beschreiben die Regeln der beiden Spiele. Aus Ihnen können wir zwar schwer vorhersagen, was das Ergebnis im nächsten Spiel sein wird, allerdings können wir eine große Anzahl von Spielen beschreiben.\nDie Funktion, die jedes Ergebnis einer Zahl zwischen 0 und 1 zuordnet nennen wir eine Warhscheinlichkeitsfunktion. Die Tabelle oben können wir als Wahrscheinlichkeitsfunktionen für die Spiele X und Y schreiben.\n\np_X(x) = \\begin{cases}\n0.5 & \\text{für } x = 0 \\\\\n0.3 & \\text{für } x = 1 \\\\\n0.2 & \\text{für } x = 2 \\\\\n\\end{cases}\n\n\np_Y(x) = \\begin{cases}\n0.5 & \\text{für } x = 0 \\\\\n0.1 & \\text{für } x = 3 \\\\\n0.4 & \\text{für } x = 4 \\\\\n\\end{cases}\n\n\n\n\n\n\n\nWarnung\n\n\n\nDie Wahrscheinlichkeitsfunktionen müssen die folgenden Bedingungen erfüllen:\n\nDie Wahrscheinlichkeiten müssen zwischen 0 und 1 liegen.\nDie Summe der Wahrscheinlichkeiten muss 1 ergeben. Für eine Zufallsvariable X mit den möglichen Ergebnissen x_1, x_2, \\ldots, x_n und den Wahrscheinlichkeiten p_1, p_2, \\ldots, p_n bedeutet das:\n\n\n\\sum_{i=1}^n p(x_i) = 1\n\n\n\nSehr häufig möchten wir diese Funktionen zusammenfassen. Zum Beispiel, aus Sicht des Spielveranstalters müssen wir entscheiden, welchen Preis für ein Spielticket zu setzen. Um das machen zu können müssen wir wissen wie viel wir im Durchschnitt zahlen werden müssen, falls wir eine große Anzahl von Tickets verkaufen.\nIn der Simulation oben haben wir gesehen, daß im Spiel X die Wahrscheinlichkeiten (0.5, 0.3, 0.2) bedeuten, daß aus 1000 Spielen ungefähr 500 Tickets keinen Gewinn bringen. Für ungefähr 300 Tickets werden wir 1 EUR zahlen müssen und für ungefähr 200 Tickets werden wir 2 EUR zahlen müssen.\nInsgesamt werden wir ungefähr\n\n500 \\times 0 + 300 \\times 1 + 200 \\times 2 = 700\n\nEuro zahlen müssen. Wenn wir die Gesamtzahlung durch die Anzahl der verkauften Tickets dividieren, erhalten wir die durchschnittliche Zahlung pro Ticket, die wir leisten müssen.\n\n\\frac{700}{1000} = 0.7\n\nDiese Zahl ist der Erwartungswert des Spieles X. Anders geschrieben:\n\n\\begin{align*}\n\\frac{500 \\times 0 + 300 \\times 1 + 200 \\times 2}{1000} =  \\\\\n0 \\times \\frac{500}{1000} + 1 \\times \\frac{300}{1000} + 2 \\times \\frac{200}{1000} = \\\\\n0 \\times 0.5 + 1 \\times 0.3 + 2 \\times 0.2 = \\\\\n\\end{align*}\n\n\nDefinition A.1 (Erwartungswert) Formal ist der Erwartungswert die gewichtete Summe der möglichen Ergebnisse. Für eine Zufallsvariable X mit den möglichen Ergebnissen x_1, x_2, \\ldots, x_n und den Wahrscheinlichkeiten p_1, p_2, \\ldots, p_n ist der Erwartungswert\n\nE(X) = x_1 \\cdot p_1 + x_2 \\cdot p_2 + \\ldots + x_n \\cdot p_n = \\sum_{i=1}^n x_i \\cdot p_i\n\n\n\n# Der Erwartungswert von X\n\nE_X = np.sum(games[\"x\"] * games[\"p_x\"])\nE_X\n\n0.7\n\n\n\nE_Y = np.sum(games[\"y\"] * games[\"p_y\"])\nE_Y\n\n0.9\n\n\nVergleichen wir diese Erwartungswerte mit den beobachteten Mittelwerten in den Simulationen\n\nnp.mean(x)\n\n0.727\n\n\n\nnp.mean(y)\n\n0.909",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Diskrete Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "Appendices/02-Random-Variables.html#eigenschaften-des-erwartungswertes",
    "href": "Appendices/02-Random-Variables.html#eigenschaften-des-erwartungswertes",
    "title": "Anhang A — Diskrete Zufallsvariablen",
    "section": "A.3 Eigenschaften des Erwartungswertes",
    "text": "A.3 Eigenschaften des Erwartungswertes\nDer Erwartungswert ist ein linearer Operator. Das bedeutet, daß der Erwartungswert einer Summe von Zufallsvariablen gleich der Summe der Erwartungswerte ist.\n\nTheorem A.1 Es seien X und Y zwei Zufallsvariablen und a und b zwei Konstanten (reale Zahlen). Dann gilt\n\nE(aX + bY) = aE(X) + bE(Y)\n\n\n\n\n\n\n\n\nBeweis\n\n\n\n\n\nFür den Beweis verwenden wir die Definition des Erwartungswertes und die Tatsache, daß die Konstante a aus der Summe herausgezogen werden kann (sie hängt nicht von dem Index der Summe ab).\n\n\\begin{align*}\nE(aX) = \\sum_{i=1}^n a x_i p_i = a \\sum_{i=1}^n x_i p_i = aE(X) \\\\\n\\end{align*}\n\nFür den zweiten Teil des Beweises brauchen wir die gemeinsame Wahrscheinlichkeitsfunktion (p_{X,Y}) von X und Y, die in Kapitel A.5 eingeführt wird. Sie können den Beweis zuerst überspringen und später zurückkehren.\n\n\\begin{align*}\nE(X + Y) = \\sum_{x}\\sum_{y} (x + y) p_{X,Y}(x, y) = \\sum_{x}\\sum_{y} (x p_{X,Y}(x, y) + y p_{X,Y}(x, y))\n\\end{align*}\n\nWir können die Summe in zwei Teile aufteilen.\n\n\\begin{align*}\nE(X + Y) = \\sum_{x}\\sum_{y} x p_{X,Y}(x, y) + \\sum_{x}\\sum_{y} y p_{X,Y}(x, y)\n\\end{align*}\n\nNun sollten wir merken, dass es in beiden Summen Komponenten gibt, die wir ausklammern können.\n\n\\begin{align*}\nE(X + Y) = \\sum_{x} x \\sum_{y} p_{X,Y}(x, y) + \\sum_{y} y \\sum_{x} p_{X,Y}(x, y)\n\\end{align*}\n\nDie Ausdrücke\n\n\\begin{align*}\n\\sum_{y} p_{X,Y}(x, y) = p_X(x) \\\\\n\\sum_{x} p_{X,Y}(x, y) = p_Y(y)\n\\end{align*}\n\nsind die Randverteilungen von X und Y. Sie sind in Definition A.3 eingeführt. Wir können sie in den Ausdrücken oben ersetzen.\nAm Ende erhalten wir\n\n\\begin{align*}\nE(X + Y) = \\sum_{x} x p_X(x) + \\sum_{y} y p_Y(y) = E(X) + E(Y)\n\\end{align*}",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Diskrete Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "Appendices/02-Random-Variables.html#die-varianz",
    "href": "Appendices/02-Random-Variables.html#die-varianz",
    "title": "Anhang A — Diskrete Zufallsvariablen",
    "section": "A.4 Die Varianz",
    "text": "A.4 Die Varianz\nDie Varianz ist ein Maß für die Streuung von Werten. Wir haben in der deskriptiven Statistik die empirische Varianz kennengelernt. Die empirische Varianz ist ein Maß für die Streuung von Werten um den Mittelwert. Die Varianz einer Verteilung (Zufallsvariable) ist ein Maß für die Streuung der Werte, die diese Verteilung produzieren wird (wenn wir das Spiel spielen).\n\nDefinition A.2 (Varianz) Die Varianz einer Zufallsvariable X mit den möglichen Ergebnissen x_1, x_2, \\ldots, x_n und den Wahrscheinlichkeiten p_1, p_2, \\ldots, p_n ist\n\n\\text{Var}(X) = E\\left((X - E(X))^2\\right) = \\sum_{i=1}^n (x_i - E(X))^2 \\cdot p_i\n\n\n\n# Lasst uns die Varianz von X berechnen\n\n# Den Erwartungswert von X haben wir bereits berechnet und unter E_X gespeichert\n\n# Für die Varianz von X berechnen wir Differenz der möglichen Ergebnisse von X\n# zum Erwartungswert\n\nx_diff_erwartungswert = games[\"x\"] - E_X\n\n# Danach quadrieren wir die Differenzen\nx_diff_erwartungswert_squared = x_diff_erwartungswert ** 2\n\n# Am ende multiplizieren wir die quadrierten Differenzen mit den Wahrscheinlichkeiten für X=0, X=1 und X=2\n\nvar_X = np.sum(x_diff_erwartungswert_squared * games[\"p_x\"])\nvar_X\n\n0.6100000000000001\n\n\n\n# Das gleiche können wir für Y machen, diesmal ohne die Zwischenschritte\n\nvar_Y = np.sum(games[\"p_y\"] * (games[\"y\"] - E_Y) ** 2)\nvar_Y\n\n0.8900000000000001\n\n\nDie Varianz von Y ist größer als die Varianz von X. Das bedeutet, daß die Werte, die Y produziert, weiter von ihrem Erwartungswert entfernt sein werden als die Werte, die X produziert.\nSehr häufig ist der folgende Satz nützlich\n\nTheorem A.2 (Varianzzerlegung) Die Varianz einer Zufallsvariable X kann in zwei Teile zerlegt werden:\n\n\\text{Var}(X) = E(X^2) - (E(X))^2\n\n\n\nBeweis. Der Beweis ist einfach. Wir beginnen mit der Definition der Varianz, danach schreiben wir die quadratische Formel um und benutzen die Linearität (Theorem A.1) des Erwartungswertes.\n\n\\begin{align*}\n\\text{Var}(X) & = E\\left((X - E(X))^2\\right) \\\\\n& = E(X^2 - 2X \\cdot E(X) + (E(X))^2) \\\\\n& = E(X^2) - 2E(X) \\cdot E(X) + (E(X))^2 \\\\\n& = E(X^2) - (E(X))^2\n\\end{align*}",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Diskrete Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "Appendices/02-Random-Variables.html#sec-joint-distribution",
    "href": "Appendices/02-Random-Variables.html#sec-joint-distribution",
    "title": "Anhang A — Diskrete Zufallsvariablen",
    "section": "A.5 Die gemeinsame Verteilung",
    "text": "A.5 Die gemeinsame Verteilung\nBisher haben wir uns nur mit einer Zufallsvariablen beschäftigt. Stellen wir uns vor, daß wir ein Spiel spielen, das zwei Ergebnisse produziert. Zum Beispiel können wir zwei Münzen werfen. Die Ergebnisse können sein: (Kopf, Kopf), (Kopf, Zahl), (Zahl, Kopf), (Zahl, Zahl). Oder aber wir spielen die Spiele X und Y und die Ergebnisse können sein (0, 0), (0, 3), (0, 4), (1, 0), (1, 3), (1, 4), (2, 0), (2, 3), (2, 4). Genau wir im ein-dimensionalen Fall können wir die Wahrscheinlichkeiten der Ergebnisse in einer Tabelle zusammenfassen.\nUm ein Beispiel zu geben, werden wir Wahrscheinlichkeiten für dieses Spiel ausdenken.\n\nimport itertools\nnp.random.seed(322)\n\n# Wir erzeugen 9 Zufallszahlen\njoint_probs = np.random.rand(9)\n\ngames2 = pd.DataFrame(itertools.product([0, 1, 2], [0, 1, 2]), columns=[\"x\", \"y\"])\n# Damit diese Zufallszahlen die Bedingungen einer Wahrscheinlichkeitsverteilung erfüllen,\n# müssen sie normiert werden, damit sie sich zu 1 summieren\n\ngames2[\"p_xy\"] =  joint_probs / joint_probs.sum()\ngames2\n\n\n\n\n\n\n\n\n\nx\ny\np_xy\n\n\n\n\n0\n0\n0\n0.198694\n\n\n1\n0\n1\n0.068238\n\n\n2\n0\n2\n0.132942\n\n\n3\n1\n0\n0.080278\n\n\n4\n1\n1\n0.008387\n\n\n5\n1\n2\n0.278337\n\n\n6\n2\n0\n0.095935\n\n\n7\n2\n1\n0.051734\n\n\n8\n2\n2\n0.085454\n\n\n\n\n\n\n\n\nIn diesem Spiel ist das wahrscheinlichste Ergebnis (1, 2) mit einer Wahrscheinlichkeit von 0.278.\nSehr häufig werden Sie diese Art von Tabelle in der Statistik sehen. Sie wird als gemeinsame Verteilung bezeichnet und wird oft in Tabellen dargestellt, wo die Werte des ersten Ergebnißes in den Zeilen und die Werte des zweiten Ergebnisses in den Spalten stehen. In den Zellen der Tabelle stehen die Wahrscheinlichkeiten.\n\ngames2_wide = games2.pivot(index=\"x\", columns=\"y\", values=\"p_xy\")\ngames2_wide\n\n\n\n\n\n\n\n\ny\n0\n1\n2\n\n\nx\n\n\n\n\n\n\n\n0\n0.198694\n0.068238\n0.132942\n\n\n1\n0.080278\n0.008387\n0.278337\n\n\n2\n0.095935\n0.051734\n0.085454\n\n\n\n\n\n\n\n\nDiese Ansicht is nützlich, denn sie erlaubt uns die Randverteilungen einfach (per Hand) zu berechnen. Die Randverteilungen sind die Verteilungen der einzelnen Zufallsvariablen. In unserem Beispiel sind die Randverteilungen die Verteilungen von X und Y.\nDie Randverteilung beantwortet die Frage: “Was ist die Wahrscheinlichkeit, daß das erste Ergebnis 1 ist?” oder “Was ist die Wahrscheinlichkeit, daß das zweite Ergebnis 2 ist?”, usw.\nIn der Tabelle oben kann X = 1 in drei verschiedenen Fällen eintreten: (1, 0), (1, 1), und (1, 2). Die Wahrscheinlichkeit, daß X = 1 ist, ist die Summe der Wahrscheinlichkeiten dieser drei Fälle, denn die Ereignisse sind disjunkt.\nDeswegen ist die Wahrscheinlichkeit, daß X = 0 ist, gleich 0.1986 + 0.0682 + 0.132942. Dieselbe Überlegung führ uns zu der Wahrscheinlichkeit, daß X = 1 ist: 0.080278 + 0.008387 + 0.278337 und zu der Wahrscheinlichkeit, daß X = 2 ist: 0.095935 + 0.051734 + 0.085454.\nGanz allgemein ergibt sich die Randverteilung von X aus der gemeinsamen Verteilung, indem die Wahrscheinlichkeiten in den Zeilen addiert werden.\nWenn wir die Tabelle spaltenweise addieren, erhalten wir die Randverteilung von Y.\n\nDefinition A.3 (Randverteilungen) Betrachten wir ein gemeinsame Wahrscheinlichkeitsfunktion p_{X,Y}(x, y) für zwei Zufallsvariablen X und Y. Die Randverteilungen von X und Y sind\n\np_X(x) = \\sum_{y} p_{X,Y}(x, y)\n\nund\n\np_Y(y) = \\sum_{x} p_{X,Y}(x, y)\n\n\nIn Python ist es einfacher, die lange Form der Tabelle zu nutzen\n\ngames2.groupby(\"x\").sum() # Dir Randverteilung von X\n\n\n\n\n\n\n\n\n\ny\np_xy\n\n\nx\n\n\n\n\n\n\n0\n3\n0.399875\n\n\n1\n3\n0.367002\n\n\n2\n3\n0.233123\n\n\n\n\n\n\n\n\n\ngames2.groupby(\"y\").sum() # Die Randverteilung von Y\n\n\n\n\n\n\n\n\n\nx\np_xy\n\n\ny\n\n\n\n\n\n\n0\n3\n0.374907\n\n\n1\n3\n0.128359\n\n\n2\n3\n0.496734",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Diskrete Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "Appendices/02-Random-Variables.html#unabhängigkeit",
    "href": "Appendices/02-Random-Variables.html#unabhängigkeit",
    "title": "Anhang A — Diskrete Zufallsvariablen",
    "section": "B.1 Unabhängigkeit",
    "text": "B.1 Unabhängigkeit\n\nDefinition B.1 (Unabhängigkeit) Zwei Zufallsvariablen X und Y nennen wir unabhängig, falls die gemeinsame Verteilung das Produkt der Randverteilungen ist.\n\np_{X,Y}(x, y) = p_X(x) \\cdot p_Y(y)\n\n\nFür unabhängige Zufallsvariablen gilt, daß die bedingten Verteilungen\nZwei Zufallsvariablen X und Y nennen wir unabhängig, wenn die bedingten Verteilungen von Y gleich der Randverteilung von Y sind (dasselbe gilt auch für X).\n\nTheorem B.1 (Unabhängigkeit und bedingte Verteilungen) Unter Unabhängigkeit sind die bedingten Verteilungen von Y für jeden Wert von X gleich der Randverteilung von Y.\n\np_{Y|X}(y|x) = p_Y(y)\n\nfür alle x.\n\n\nBeweis. Die Randverteilung von Y erhalten wir, indem wir die gemeinsame Verteilung über die möglichen Werte von X summieren.\n\np_Y(y) = \\sum_{x} p_{X,Y}(x, y)\n\nDie bedingte Verteilung von Y gegeben X erhalten wir, indem wir die gemeinsame Verteilung durch die Randverteilung von X teilen.\n\np_{Y|X}(y|x) = \\frac{p_{X,Y}(x, y)}{p_X(x)}\n\nDa sich laut Definition B.1 die gemeinsame Verteilung als Produkt der Randverteilungen schreiben lässt, erhalten wir\n\np_{Y|X}(y|x) = \\frac{p_X(x)p_Y(y)}{p_X(x)} = p_Y(y)\n\n\n\n# Hier konstruieren wir die gemeinsame Verteilung von den Randverteilungen der \n# beiden Spiel X und Y\n\nxy = list(itertools.product([0, 1, 2], [0, 1, 2]))\nprobs = list(itertools.product([0.5, 0.3, 0.2], [0.5, 0.1, 0.4]))\n\n# Wir multiplizieren die Randverteilungen von X und Y um die gemeinsame Verteilung zu erhalten\njoint_probs = [p_x * p_y for p_x, p_y in probs]\n\ngames_ind = pd.DataFrame(xy, columns=[\"x\", \"y\"])\ngames_ind[\"p_xy\"] = joint_probs\ngames_ind\n\n\n\n\n\n\n\n\n\nx\ny\np_xy\n\n\n\n\n0\n0\n0\n0.25\n\n\n1\n0\n1\n0.05\n\n\n2\n0\n2\n0.20\n\n\n3\n1\n0\n0.15\n\n\n4\n1\n1\n0.03\n\n\n5\n1\n2\n0.12\n\n\n6\n2\n0\n0.10\n\n\n7\n2\n1\n0.02\n\n\n8\n2\n2\n0.08\n\n\n\n\n\n\n\n\n\n# Lasst uns die bedingten Verteilungen von Y gegeben X berechnen (für jedes x)\n\ngames_ind[\"p_y_given_x\"] = games_ind[\"p_xy\"] / games_ind.groupby(\"x\")[\"p_xy\"].transform('sum')\ngames_ind\n\n\n\n\n\n\n\n\n\nx\ny\np_xy\np_y_given_x\n\n\n\n\n0\n0\n0\n0.25\n0.5\n\n\n1\n0\n1\n0.05\n0.1\n\n\n2\n0\n2\n0.20\n0.4\n\n\n3\n1\n0\n0.15\n0.5\n\n\n4\n1\n1\n0.03\n0.1\n\n\n5\n1\n2\n0.12\n0.4\n\n\n6\n2\n0\n0.10\n0.5\n\n\n7\n2\n1\n0.02\n0.1\n\n\n8\n2\n2\n0.08\n0.4\n\n\n\n\n\n\n\n\nWas wir feststellen müssen ist es, daß die bedingten Verteilungen von Y gegeben X für jeden Wert von X gleich sind.\n\n# Lasst und die Randverteilung von Y berechnen\n\np_y = games_ind.groupby(\"y\")[\"p_xy\"].sum()\np_y\n\ny\n0    0.5\n1    0.1\n2    0.4\nName: p_xy, dtype: float64",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Diskrete Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "Appendices/02-Random-Variables.html#der-bedingte-erwartungswert",
    "href": "Appendices/02-Random-Variables.html#der-bedingte-erwartungswert",
    "title": "Anhang A — Diskrete Zufallsvariablen",
    "section": "B.2 Der bedingte Erwartungswert",
    "text": "B.2 Der bedingte Erwartungswert\nBisher haben wir den Erwartungswert von Zufallsvariablen in Bezug auf ihre Randverteilungen berechnet. Wir können auch den Erwartungswert in Bezug auf die bedingte Verteilung berechnen. Diesen Erwartungswert nennen wir den bedingten Erwartungswert.\n\nDefinition B.2 (Bedingter Erwartungswert) Der bedingte Erwartungswert von Y gegeben X = x ist\n\nE(Y|X = x) = \\sum_{y} y \\cdot p_{Y|X}(y|x)\n\n\nIm Fall von unabhängigen Zufallsvariablen sind die bedingten Erwartungswerte gleich dem (unbedingten) Erwartungswert. Das ist eine direkte Folge aus der Tatsache, daß die bedingten Verteilungen gleich der Randverteilung sind.\n\n# Als Beispiel berechnen wir die bedingten Erwartungswerten von Y gegeben X \n# in den zwei Beispielen, die wir oben hatten\n\nE_Y_given_X = games2.groupby(\"x\").apply(lambda gr: np.sum(gr[\"y\"] * gr[\"p_y_given_x\"]))\nE_Y_given_X\n\n/tmp/ipykernel_2467/2565904380.py:4: DeprecationWarning:\n\nDataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n\n\n\nx\n0    0.835568\n1    1.539668\n2    0.955041\ndtype: float64\n\n\nDer bedingte Erwartungswert von Y ist bei X = 2 am größten und bei X = 0 am kleinsten. Das heißt, dass sich unsere Erwartungen über den Gewinn aus Spiel Y ändern, wenn wir das Ergebnis von Spiel X kennen.\n\n# Im Fall der unabhängigen Zufallsvariablen X und Y in games_ind\n\nE_Y_given_X_ind = games_ind.groupby(\"x\").apply(lambda gr: np.sum(gr[\"y\"] * gr[\"p_y_given_x\"]))\nE_Y_given_X_ind\n\n/tmp/ipykernel_2467/3047749272.py:3: DeprecationWarning:\n\nDataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n\n\n\nx\n0    0.9\n1    0.9\n2    0.9\ndtype: float64\n\n\nIm Fall von unabhängigen Zufallsvariablen ist der bedingte Erwartungswert gleich dem (unbedingten) Erwartungswert. Das heißt, dass wir aus der Kenntnis des Ergebnisses von X keine zusätzliche Information über das Ergebnis von Y erhalten.",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Diskrete Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "Appendices/02-Random-Variables.html#die-kovarianz",
    "href": "Appendices/02-Random-Variables.html#die-kovarianz",
    "title": "Anhang A — Diskrete Zufallsvariablen",
    "section": "B.3 Die Kovarianz",
    "text": "B.3 Die Kovarianz\nWir haben bisher die Randverteilungen von Zufallsvariablen durch den Erwartungswert (Lage) und durch die Varianz (Streuung) zusammengefasst. Wir können auch die gemeinsame Verteilung von zwei Zufallsvariablen hinsichtlich des Zusammenhangs zwischen den beiden Variablen zusammenfassen.\n\nDefinition B.3 (Kovarianz) \n\\text{Cov}(X, Y) = E_{XY}((X - E(X)) \\cdot (Y - E(Y)))\n\nWir schreiben E_{XY}, um zu betonen, daß wir den Erwartungswert über die gemeinsame Verteilung berechnen.\n\nUm die Kovarianz berechnen zu können ist oftmals der folgende Satz nützlich:\n\nTheorem B.2 (Zerlegung der Varianz) \n\\text{Cov}(X, Y) = E_{XY}(X \\cdot Y) - E(X) \\cdot E(Y)\n :::{.proof}\nDer Satz ist sehr leicht zu beweisen. Wir müssen nur die Definition der Kovarianz und die Definition des Erwartungswertes benutzen.\n\n\\begin{align*}\n\\text{Cov}(X, Y) & = E_{XY}((X - E(X)) \\cdot (Y - E(Y))) \\\\\n& = E_{XY}(X \\cdot Y - X \\cdot E(Y) - E(X) \\cdot Y + E(X) \\cdot E(Y)) \\\\\n& = E_{XY}(X \\cdot Y) - E_{XY}(X \\cdot E(Y)) - E_{XY}(E(X) \\cdot Y) + E_{XY}(E(X) \\cdot E(Y)) \\\\\n& = E_{XY}(X \\cdot Y) - E(X) \\cdot E(Y)\n\\end{align*}\n\n\n\n# Berechnen wir die Kovarianz für die zwei Spiele in games2\n\ngames2[\"xy\"] = games2[\"x\"] * games2[\"y\"]\nE_XY = np.sum(games2[\"xy\"] * games2[\"p_xy\"])\n\ncov_XY = E_XY - E_X * E_Y\ncov_XY\n\n0.38034575941137405\n\n\n\n# Berechnen wir die Kovarianz für die zwei Spiele in games_ind\n\ngames_ind[\"xy\"] = games_ind[\"x\"] * games_ind[\"y\"]\nE_XY_ind = np.sum(games_ind[\"xy\"] * games_ind[\"p_xy\"])\nE_X_ind = np.sum(games[\"x\"] * games[\"p_x\"])\n\nE_Y_ind = np.sum(games[\"y\"] * games[\"p_y\"])\ncov_XY_ind = E_XY_ind - E_X_ind * E_Y_ind\ncov_XY_ind\n\n1.1102230246251565e-16\n\n\nWas wir uns hier merken können ist es, dass die Kovarianz im Fall von unabhängigen Zufallsvariablen null ist. Die Umkehrung ist jedoch nicht wahr. Es ist durchaus möglich, daß zwei Variablen eine Kovarianz von null haben, die allerdings nicht unabhängig sind.\n\nTheorem B.3 Es seien X und Y zwei unabhängige Zufallsvariablen. Dann ist ihre Kovarianz gleich null.\n\n\\text{Cov}(X, Y) = 0\n\n\n\nBeweis. Wir starten mit der Zerlegung der Kovarianz aus Theorem B.2 und benutzen die Tatsache, daß die gemeinsame Verteilung das Produkt der Randverteilungen ist. Wir werden den Satz im diskreten Fall beweisen, allerdings gilt der Satz auch für stetige Verteilungen.\n\n\\begin{align*}\n\\text{Cov}(X, Y) & = E_{XY}(X \\cdot Y) - E_X(X) \\cdot E_Y(Y)\n\\end{align*}\n\nDer Erwartungswert des Produktes von X und Y ist einfach die Summe aller möglichen Produkte der Werte von X und Y multipliziert mit den Wahrscheinlichkeiten, daß diese Werte eintreten. Wir werden die Tatsache benutzen, daß die gemeinsame Verteilung das Produkt der Randverteilungen ist.\n\np_{XY}(x, y) = p_X(x) \\cdot p_Y(y)\n\n\n\\begin{align*}\nE_{XY}(X \\cdot Y) & = \\sum_{x} \\sum_{y} x \\cdot y \\cdot p_{XY}(x, y) x \\cdot y \\\\\n& = \\sum_{x} \\sum_{y} x \\cdot y \\cdot p_X(x) \\cdot p_Y(y) \\\\\n& = \\left(\\sum_{x} x \\cdot p_X(x) \\right) \\left(\\sum_{y} y \\cdot p_Y(y)\\right) \\\\\n& = E_X(X) \\cdot E_Y(Y)\n\\end{align*}",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Diskrete Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "Appendices/03-Die-Normalverteilung.html",
    "href": "Appendices/03-Die-Normalverteilung.html",
    "title": "Anhang B — Die Normalverteilung",
    "section": "",
    "text": "B.1 Diskrete Verteilungen",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Die Normalverteilung</span>"
    ]
  },
  {
    "objectID": "Appendices/03-Die-Normalverteilung.html#the-normal-distribution",
    "href": "Appendices/03-Die-Normalverteilung.html#the-normal-distribution",
    "title": "Anhang B — Die Normalverteilung",
    "section": "B.2 The Normal Distribution",
    "text": "B.2 The Normal Distribution\nThe normal distribution is a continuous probability distribution. It is also called the Gaussian distribution. The probability density function (pdf) of the normal distribution is given by:\n\nf(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}\\\\\n\nwhere \\mu is the mean and \\sigma is the standard deviation. The normal distribution is symmetric around the mean \\mu. The standard normal distribution has \\mu = 0 and \\sigma = 1.\nInstead of writing the density function every time we want to refer to the normal distribution, we use the notation X \\sim N(\\mu, \\sigma^2) to denote that the random variable X has a normal distribution with mean \\mu and variance \\sigma^2.\n\nX \\sim N(\\mu, \\sigma^2) \\\\\nE(X) = \\mu \\\\\nVar(X) = \\sigma^2 \\\\\nSD(X) = \\sqrt{\\sigma^2} = \\sigma\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplayers_n = 300\ngames_n = 16\n\n# Create a DataFrame similar to expand_grid in R\nunif_games = pd.DataFrame(\n    np.array(\n        np.meshgrid(\n            np.arange(1, games_n + 1),\n            np.arange(1, players_n + 1)\n        )).T.reshape(-1, 2),\n    columns=['game', 'player']\n)\n\n# Add result column with random uniform values between -1 and 1\nunif_games['result'] = np.random.uniform(-1, 1, size=len(unif_games))\n\n# Add initial values for each player\ninitial_values = pd.DataFrame(\n    {'player': np.arange(1, players_n + 1), 'game': 0, 'result': 0})\nunif_games = pd.concat([unif_games, initial_values])\n\n# Sort values and calculate running total for each player\nunif_games = unif_games.sort_values(['player', 'game'])\nunif_games['running_total'] = unif_games.groupby('player')['result'].cumsum()\n\n# Plotting\nplt.figure(figsize=(10, 6))\nfor player in unif_games['player'].unique():\n    player_data = unif_games[unif_games['player'] == player]\n    plt.plot(player_data['game'], player_data['running_total'],\n             color='skyblue', alpha=0.2)\n\n# First player\nplayer_data = unif_games[unif_games['player'] == 1]\nplt.plot(player_data['game'], player_data['running_total'],\n         color='firebrick', label='Player 1')\n\nplt.axhline(0, color='black')\n\nfor mark in [4, 8, 16]:\n    plt.axvline(x=mark, linestyle='--', color='black')\n\nplt.xlabel('Game number')\nplt.ylabel('Running Total')\nplt.xticks([0, 4, 8, 12, 16])\nplt.show()",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Die Normalverteilung</span>"
    ]
  },
  {
    "objectID": "Appendices/03-Die-Normalverteilung.html#the-multivariate-normal-distribution",
    "href": "Appendices/03-Die-Normalverteilung.html#the-multivariate-normal-distribution",
    "title": "Anhang B — Die Normalverteilung",
    "section": "B.3 The Multivariate Normal Distribution",
    "text": "B.3 The Multivariate Normal Distribution\nThe multivariate normal distribution is a generalization of the one-dimensional normal distribution to higher dimensions. The probability density function (pdf) of the multivariate normal distribution is given by:\n\nf(x) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} e^{-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}\\\\\n\nwhere \\mu is the mean vector and \\Sigma is the covariance matrix. The multivariate normal distribution is symmetric around the mean \\mu. The standard multivariate normal distribution has \\mu = 0 and \\Sigma = I, where I is the identity matrix.\n\n\nCode\n# Importing the necessary modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\n \n \n# plt.style.use('seaborn-dark')\nplt.rcParams['figure.figsize']=14, 6\nfig = plt.figure()\n \n# Initializing the random seed\nrandom_seed=1000\n \n# List containing the variance\n# covariance values\ncov_val = [-0.8, 0, 0.8]\n \n# Setting mean of the distributino \n# to be at (0,0)\nmean = np.array([0,0])\n \n# Storing density function values for \n# further analysis\npdf_list = []\n \n# Iterating over different covariance values\nfor idx, val in enumerate(cov_val):\n     \n    # Initializing the covariance matrix\n    cov = np.array([[1, val], [val, 1]])\n     \n    # Generating a Gaussian bivariate distribution\n    # with given mean and covariance matrix\n    distr = multivariate_normal(cov = cov, mean = mean,\n                                seed = random_seed)\n     \n    # Generating a meshgrid complacent with\n    # the 3-sigma boundary\n    mean_1, mean_2 = mean[0], mean[1]\n    sigma_1, sigma_2 = cov[0,0], cov[1,1]\n     \n    x = np.linspace(-3*sigma_1, 3*sigma_1, num=100)\n    y = np.linspace(-3*sigma_2, 3*sigma_2, num=100)\n    X, Y = np.meshgrid(x,y)\n     \n    # Generating the density function\n    # for each point in the meshgrid\n    pdf = np.zeros(X.shape)\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            pdf[i,j] = distr.pdf([X[i,j], Y[i,j]])\n     \n    # Plotting the density function values\n    key = 131+idx\n    ax = fig.add_subplot(key, projection = '3d')\n    ax.plot_surface(X, Y, pdf, cmap = 'viridis')\n    plt.xlabel(\"x1\")\n    plt.ylabel(\"x2\")\n    plt.title(f'Covariance between x1 and x2 = {val}')\n    pdf_list.append(pdf)\n    ax.axes.zaxis.set_ticks([])\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\n\n# Parameters to set for Gaussian distribution\nmu_x = 0\nsigma_x = np.sqrt(5)\nmu_y = 0\nsigma_y = np.sqrt(5)\n\n# Create grid and multivariate normal\nx = np.linspace(-10, 10, 500)\ny = np.linspace(-10, 10, 500)\nX, Y = np.meshgrid(x, y)\nMVN = multivariate_normal(mean=[mu_x, mu_y], cov=[[sigma_x, 0], [0, sigma_y]])\n\npos = np.empty(X.shape + (2,))\npos[:, :, 0] = X\npos[:, :, 1] = Y\n\nZ = MVN.pdf(pos)\n\n# Create plane\nx_p = 0\ny_p = np.linspace(-10, 10, 500)\nz_p = np.linspace(0, 0.02, 500)\nY_p, Z_p = np.meshgrid(y_p, z_p)\n\n# Finding closest idx values of X mesh to x_p\ntol = 1e-4\nidx_x_p = (np.where(x &lt; x_p+tol) and np.where(x &gt; x_p-tol))[0][0]\n# Select the corresponding values of X, Y, Z (carefully switch X and Y)\nx_c, y_c, z_c = Y[idx_x_p], X[idx_x_p], Z[idx_x_p]\n\n# Plot\nfig = plt.figure(figsize=plt.figaspect(0.5))\nax = fig.add_subplot(1, 1, 1, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', linewidth=0, zorder=0)\nax.plot_surface(x_p, Y_p, Z_p, color='r', linewidth=0, alpha=0.5, zorder=5)\nax.plot(x_c, y_c, z_c, zorder=10)\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nCode\nSMVN = multivariate_normal(mean=[0, 0], cov=[[1,0.5],[0.5,1]])\n\na = np.array([-5, 0, 5])\nb = np.array([-5, 0, 5])\n\nA, B = np.meshgrid(a, b)\nprint(A)\nprint(B)\n\n\n[[-5  0  5]\n [-5  0  5]\n [-5  0  5]]\n[[-5 -5 -5]\n [ 0  0  0]\n [ 5  5  5]]\n\n\n\n\nCode\npos = np.empty(A.shape + (2,))\npos[:, :, 0] = A\npos[:, :, 1] = B\n\nSMVN.cdf(pos)\n\n\narray([[8.24708643e-10, 2.86239218e-07, 2.86651572e-07],\n       [2.86239218e-07, 3.33333333e-01, 5.00000000e-01],\n       [2.86651572e-07, 5.00000000e-01, 9.99999428e-01]])\n\n\n\n\nCode\nSMVN.pdf([-1, -1])\nSMVN.pdf([0, -1])\n\n\n0.09435389770895924\n\n\n\n\nCode\ntmp = np.array(\n    np.meshgrid(\n        np.arange(1, games_n + 1),\n        np.arange(1, players_n + 1)\n    )\n)\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\n\n# Define the mean and covariance matrix for the bivariate normal distribution\nmean = np.array([0, 0])\ncov = np.array([[1, 0], [0, 1]])\n\n# Create a grid of values\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\n\n# Compute the bivariate normal probabilities over the grid\nrv = multivariate_normal(mean, cov)\nZ = rv.pdf(np.dstack((X, Y)))\n\n# Plot the bivariate normal probabilities\nplt.contourf(X, Y, Z)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.stats import multivariate_normal\nfrom scipy.integrate import dblquad\n\n# Define the mean and covariance matrix for the bivariate normal distribution\nmean = np.array([0, 0])\ncov = np.array([[1, 0], [0, 1]])\n\n# Define the bivariate normal distribution\nrv = multivariate_normal(mean, cov)\n\n# Define the limits of the interval\nx_interval = [0, 1]\ny_interval = [0, 1]\n\n# Define the function to integrate\nfunc = lambda x, y: rv.pdf([x, y])\n\n# Compute the probability over the interval using numerical integration\nprob, _ = dblquad(func, x_interval[0], x_interval[1], lambda x: y_interval[0], lambda x: y_interval[1])\n\nprint(f\"The probability over the interval is {prob}\")\n\n\nThe probability over the interval is 0.11651623566859808\n\n\n\n\nCode\ndef discretize_bivariate_normal(mean, cov, x_intervals, y_intervals):\n    # Define the bivariate normal distribution\n    rv = multivariate_normal(mean, cov)\n\n    grid = np.array(\n        np.meshgrid(\n            np.array(x_intervals + [np.inf]),\n            np.array(y_intervals + [np.inf])\n        )\n    ).T.reshape(-1, 2)\n\n    # Compute the probability over the interval using numerical integration\n\n    x_int = [-np.inf] + x_intervals + [np.inf]\n    y_int = [-np.inf] + y_intervals + [np.inf]\n\n    probs = np.zeros((len(x_int) - 1, len(y_int) - 1))\n\n\n    for i in range(len(x_int) - 1):\n        for j in range(len(y_int) - 1):\n            xb_l, xb_u = x_int[i:i+2]\n            yb_l, yb_u = y_int[j:j+2]\n\n            p1 = rv.cdf([xb_u, yb_l]) if xb_u != - \\\n                np.inf and yb_l != -np.inf else 0\n            p2 = rv.cdf([xb_l, yb_u]) if xb_l != - \\\n                np.inf and yb_u != -np.inf else 0\n            p3 = rv.cdf([xb_l, yb_l]) if xb_l != - \\\n                np.inf and yb_l != -np.inf else 0\n\n            prob = rv.cdf([xb_u, yb_u]) - p1 - p2 + p3\n            probs[i, j] = prob\n\n    return grid, probs\n\n\ngr, pr = discretize_bivariate_normal(\n    np.array([0, 0]),\n    np.array([[1, 0], [0, 1]]),\n    [-1, 0],\n    [-1, 0]\n)\n\n\n\n# This can be done using the cumulative sum and subtracting the previous value\n# by grouping on the slower changing variable\n\ndf = pd.DataFrame(gr, columns=['x', 'y'])\ndf['prob'] = pr.flatten()\ndf\n\n\n\n\n\n\n\n\n\nx\ny\nprob\n\n\n\n\n0\n-1.0\n-1.0\n0.025171\n\n\n1\n-1.0\n0.0\n0.054156\n\n\n2\n-1.0\ninf\n0.079328\n\n\n3\n0.0\n-1.0\n0.054156\n\n\n4\n0.0\n0.0\n0.116516\n\n\n5\n0.0\ninf\n0.170672\n\n\n6\ninf\n-1.0\n0.079328\n\n\n7\ninf\n0.0\n0.170672\n\n\n8\ninf\ninf\n0.250000\n\n\n\n\n\n\n\n\n\n\n# # Define the bivariate normal distribution\n# rv1 = multivariate_normal(np.array([0, 0]), np.array([[1, 0], [0, 1]]))\n\n# # rv1.cdf(np.array([[0, 0], [1, 1], [np.inf, np.inf]]))\n# rv1.cdf(tmp)\n\n# import pandas as pd\n\n# df = pd.DataFrame(tmp, columns=['x', 'y'])\n# df[\"cdf\"] = rv1.cdf(tmp)\n# # df[\"cdf_diff\"] = df[\"cdf\"].diff().fillna(df[\"cdf\"])\n# df\n\n\n# df[\"cdf_d_gr\"] = df.groupby('x')[\"cdf\"].transform(\"diff\").fillna(df[\"cdf\"])\n# df[\"cdf_d_gr_cs\"] = df.groupby('x')[\"cdf_d_gr\"].transform(\"cumsum\")\n# df\n\n\n# np.array(\n#     np.meshgrid(\n#         np.array([1, 2, 3]),\n#         np.array([4, 5])\n#     )\n# )",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Die Normalverteilung</span>"
    ]
  },
  {
    "objectID": "Appendices/04-LA.html",
    "href": "Appendices/04-LA.html",
    "title": "Anhang C — Elemente der Linearen Algebra",
    "section": "",
    "text": "C.1 Vektoren\nIn der Geometrie sind werden (endliche) Sequenzen von realen Zahlen Vektoren genannt. Wir schreiben Vektoren in der Regel als Spaltenvektoren, also als n \\times 1-Matrizen. Ein Vektor v mit n Einträgen wird also geschrieben als\nv = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix}\nwobei v_1, v_2, \\ldots, v_n die Elemente des Vektors sind.\nBeispiele für zwei-dimensionale Vektoren sind\n\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\nBeispiele für drei-dimensionale Vektoren sind\n\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}, \\quad \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\nWir können Vektoren grafisch als Pfeile darstellen. Der Vektor \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} kann als ein Pfeil von (0, 0) nach (1, 2) dargestellt werden.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfix, ax = plt.subplots()\n\nax.quiver(0, 0, 1, 2, angles='xy', scale_units='xy', scale=1)\nax.axvline(x=0, color='gray')\nax.axhline(y=0, color='gray')\nax.set_xlim(-1, 2)\nax.set_ylim(-1, 2)\nObwohl wir üblicherweise Vektoren als Pfeile von (0, 0) zu einem anderen Punkt darstellen, enthalten Vektoren keine Information über ihren Ursprung. Sie liefern uns Information über ihre Richtung und über ihre Länge. Also können wir für den Vektor \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} auch einen Pfeil von (2, 3) nach (3, 5) zeichnen.\n# Creating arrow\nx = np.array([-1, 0, 1])\ny = np.array([2, 3])\n \nX, Y = np.meshgrid(x, y)\n\n# creating plot\nfig, ax = plt.subplots(figsize =(14, 8))\nax.quiver(X, Y, 1, 2, angles='xy', scale_units='xy', scale=1, color='r')\n \nax.set_xlim(-1.5, 3)\nax.set_ylim(1.5, 5.5)\n# show plot\nplt.show()\nWir können uns die Länge eines Vektors als die euklidische Distanz von dem Nullpunkt zu dem Punkt vorstellen, den der Vektor darstellt. Es ist einfacher, sich das in \\mathbb{R}^2 vorzustellen, aber es gilt auch in höheren Dimensionen.\nfix, ax = plt.subplots()\n\nax.quiver(0, 0, 1, 2, angles='xy', scale_units='xy', scale=1)\nax.axvline(x=0, color='gray')\nax.axhline(y=0, color='gray')\nax.set_xlim(-1, 2.2)\nax.set_ylim(-1, 2.2)\n\nax.vlines(x=1, ymin=0, ymax=2, color='r')\nax.annotate('Länge der Strecke = 2', (1.05, 1), fontsize=10, color='grey')\nax.hlines(y=0, xmin=0, xmax=1, color='r')\nax.annotate('Länge der Strecke = 1', (0.05, -0.15), fontsize=10, color='grey')\n\nax.annotate('A', (1.01, 2.01), fontsize=10, color='black')\nax.annotate('O', (0.01, 0.01), fontsize=10, color='black', textcoords=\"offset points\", xytext=(-10,1))\nax.annotate('C', (1.01, 0.01), fontsize=10, color='black', textcoords=\"offset points\", xytext=(5,1))\n\nText(5, 1, 'C')\nDie Länge der Strecke OA können wir über den Satz des Pythagoras berechnen.\n|OA| = \\sqrt{1^2 + 2^2} = \\sqrt{5}\n## Die Länge eines Vektors können wir wie folgt berechnen:\n\na = np.array([1, 2])\nlaenge = np.linalg.norm(a)\n\nprint(laenge)\nprint(np.isclose(laenge, np.sqrt(5)))\n\n2.23606797749979\nTrue",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Elemente der Linearen Algebra</span>"
    ]
  },
  {
    "objectID": "Appendices/04-LA.html#vektoren",
    "href": "Appendices/04-LA.html#vektoren",
    "title": "Anhang C — Elemente der Linearen Algebra",
    "section": "",
    "text": "Definition C.1 (Die Länge eines Vektors) Die Länge eines Vektors v = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix} ist definiert als\n\n\\|v\\| = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2} = \\sqrt{\\sum_{i=1}^n v_i^2}",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Elemente der Linearen Algebra</span>"
    ]
  },
  {
    "objectID": "Appendices/04-LA.html#addition-von-vektoren",
    "href": "Appendices/04-LA.html#addition-von-vektoren",
    "title": "Anhang C — Elemente der Linearen Algebra",
    "section": "C.2 Addition von Vektoren",
    "text": "C.2 Addition von Vektoren\nWir können zwei Vektoren miteinander addieren, indem wir ihre Elemente paarweise addieren. Wenn v = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix} und w = \\begin{pmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{pmatrix} zwei Vektoren sind, dann ist die Summe v + w definiert als\n\nv + w = \\begin{pmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\\\ \\vdots \\\\ v_n + w_n \\end{pmatrix}\n\nEs ist hilfreich, die Summe zweier Vektoren grafisch darzustellen. Wenn v = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} und w = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} sind, dann ist v + w = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}.\n\nv = np.array([1, 2])\nw = np.array([3, 1])\nv_plus_w = v + w\n\nprint(v_plus_w)\n\n[4 3]\n\n\n\nfix, ax = plt.subplots()\n\nax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1)\nax.quiver(0, 0, w[0], w[1], angles='xy', scale_units='xy', scale=1, color='r')\nax.quiver(0, 0, v_plus_w[0], v_plus_w[1], angles='xy', scale_units='xy', scale=1, color='firebrick')\n\nax.quiver(v[0], v[1], w[0], w[1], angles='xy', scale_units='xy', scale=1, color='r', alpha=0.5)\n\nax.set_xlim(0, 4)\nax.set_ylim(0, 4)\n\n\n\n\n\n\n\n\nDie Vektoren in der Geometrie können wir uns als Vorschriften vorstellen, wie wir uns von einem Punkt zu einem anderen bewegen. Wenn wir uns am Anfang im Punkt (0, 0) befinden, sagt und der Vektor v = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, dass wir uns um eine Einheit nach rechts und um zwei Einheiten nach oben bewegen sollen. Der Vektor w = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} sagt uns, dass wir uns um drei Einheiten nach rechts und um eine Einheit nach oben bewegen sollen.\nDie Summe der Vektoren sagt uns in einer kompakten Art und Weise, wie wir uns bewegen sollen, wenn wir zuerst die Vorschrift des Vektors v und dann die Vorschrift des Vektors w befolgen. Am Ende müssen wir uns um vier Einheiten nach rechts und um drei Einheiten nach oben bewegen.\nDie Differenz von zwei Vektoren v und w ist definiert als v - w = v + (-w), wobei -w der Vektor ist, der durch Multiplikation von w mit -1 entsteht. Geometrisch ist -w der Vektor, der die gleiche Länge wie w hat, aber in die entgegengesetzte Richtung zeigt.\n\nfig, ax = plt.subplots()\n\nv_min_w = v - w\n\nax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1)\nax.quiver(0, 0, w[0], w[1], angles='xy', scale_units='xy', scale=1, color='r')\nax.quiver(0, 0, -w[0], -w[1], angles='xy', scale_units='xy', scale=1, color='black')\n\nax.quiver(v[0], v[1], -w[0], -w[1], angles='xy', scale_units='xy', scale=1, color='black', alpha=0.5)\nax.quiver(0, 0, v_min_w[0], v_min_w[1], angles='xy', scale_units='xy', scale=1, color='firebrick')\nax.quiver(w[0], w[1], v_min_w[0], v_min_w[1], angles='xy', scale_units='xy', scale=1, color='firebrick')\n\nax.set_xlim(-4, 4)\nax.set_ylim(-4, 4)\n\n\n\n\n\n\n\n\nGanz wie in den Überlegungen zur Addition von Vektoren, können wir die Differenz von zwei Vektoren auch als Vorschrift interpretieren, wie wir uns von einem Punkt zu einem anderen bewegen. Wenn wir uns am Anfang im Punkt (0, 0) befinden, sagt uns der Vektor v = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, dass wir uns um eine Einheit nach rechts und um zwei Einheiten nach oben bewegen sollen. Der Vektor -w = \\begin{pmatrix} -3 \\\\ -1 \\end{pmatrix} sagt uns, dass wir uns um drei Einheiten nach links und um eine Einheit nach unten bewegen sollen.",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Elemente der Linearen Algebra</span>"
    ]
  },
  {
    "objectID": "Appendices/04-LA.html#das-skalarprodukt",
    "href": "Appendices/04-LA.html#das-skalarprodukt",
    "title": "Anhang C — Elemente der Linearen Algebra",
    "section": "C.3 Das Skalarprodukt",
    "text": "C.3 Das Skalarprodukt\nDas Skalarprodukt ist eine Zahl, die uns Auskunft darüber gibt, wie ähnlich zwei Vektoren sind. Es wird auch als inneres Produkt bezeichnet. Das Skalarprodukt zweier Vektoren ist definiert als die Summe der Produkte der entsprechenden Komponenten der beiden Vektoren. Das Skalarprodukt wird mit einem Punkt dargestellt, z.B. \\vec{a} \\cdot \\vec{b}.\n\nDefinition C.2 Das Skalarprodukt zweier Vektoren \\vec{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix} und \\vec{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{pmatrix} ist definiert als\n\n\\vec{v} \\cdot \\vec{w} = v_1w_1 + v_2w_2 + \\ldots + v_nw_n = \\sum_{i=1}^n v_iw_i\n\n\n\nBeispiel C.1 Es seien \\vec{a} = (1, 2) \\vec{b} = (4, 5) Dann ist das Skalarprodukt der beiden Vektoren:\n\\vec{a} \\cdot \\vec{b} = 1 \\cdot 4 + 2 \\cdot 5 = 4 + 10 = 14\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\na = np.array([1, 2])\nb = np.array([4, 5])\n\nprint(np.dot(a, b))\n\n14\n\n\n\nfig, ax = plt.subplots()\nplt.quiver(0, 0, a[0], a[1], angles='xy', scale_units='xy', scale=1, color='r', label='a')\nplt.quiver(0, 0, b[0], b[1], angles='xy', scale_units='xy', scale=1, color='b', label='b')\nplt.xlim(-1, 5)\nplt.ylim(-1, 6)\nplt.xlabel('x')\nplt.ylabel('y')\n\nangle = np.arccos(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\nprint(np.degrees(angle))\n\nplt.legend()\nplt.grid(True)\n\n12.094757077012119\n\n\n\n\n\n\n\n\n\nDas Skalarprodukt ist positiv, wenn der Winkel zwischen den beiden Vektoren kleiner als 90 Grad ist (spitzer Winkel). Das Skalarprodukt ist negativ, wenn der Winkel zwischen den beiden Vektoren größer als 90 Grad ist (stumpfer Winkel).\n\na1 = np.array([1, 2])\na2 = np.array([-1, 0])\n\nprint(np.dot(a1, a2))\n\n-1\n\n\n\nfig, ax = plt.subplots()\nax.quiver(0, 0, a1[0], a1[1], angles='xy', scale_units='xy', scale=1, color='r', label='a1')\nax.quiver(0, 0, a2[0], a2[1], angles='xy', scale_units='xy', scale=1, color='b', label='a2')\nax.set_xlim(-2, 2)\nax.set_ylim(-1, 3)\nax.set_xlabel('x')\nax.set_ylabel('y')\n\nText(0, 0.5, 'y')",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Elemente der Linearen Algebra</span>"
    ]
  },
  {
    "objectID": "Appendices/04-LA.html#der-kosinussatz",
    "href": "Appendices/04-LA.html#der-kosinussatz",
    "title": "Anhang C — Elemente der Linearen Algebra",
    "section": "C.4 Der Kosinussatz",
    "text": "C.4 Der Kosinussatz\nDer Kosinussatz ist ein Satz aus der Geometrie, der die Länge einer Seite eines Dreiecks in Beziehung zu den Längen der anderen beiden Seiten und dem Winkel zwischen diesen Seiten setzt. Der Kosinussatz lautet:\nc^2 = a^2 + b^2 - 2ab \\cdot \\cos(\\theta)\nwobei a, b und c die Längen der Seiten des Dreiecks sind und \\gamma der Winkel zwischen den Seiten a und b ist.\nDer Kosinussatz kann auch auf Vektoren angewendet werden. Das Skalarprodukt zweier Vektoren \\vec{a} und \\vec{b} ist definiert als:\n\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\| \\cdot \\|\\vec{b}\\| \\cdot \\cos(\\theta)\nwobei \\|\\vec{a}\\| die Länge des Vektors \\vec{a} ist und \\theta der Winkel zwischen den beiden Vektoren ist.\nDaraus folgt:\n\\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\| \\cdot \\|\\vec{b}\\|}\n\nfig, ax = plt.subplots()\n\na = np.array([1, 4])\nb = np.array([4, 5])\nc = np.array([1, 1])\n\nproj_a_bc = c + (b - c) * np.dot(a - c, b - c) / (np.linalg.norm(b - c) ** 2)\n\nax.annotate(\"A\", a)\nax.annotate(\"B\", b)\nax.annotate(\"C\", c)\n\nax.annotate(\"b\", (b + c) / 2, xytext=(10, 0), textcoords='offset points')\nax.annotate(\"a\", (a + c) / 2, xytext=(-10, 0), textcoords='offset points')\nax.annotate(\"c\", (a + b) / 2, xytext=(0, 10), textcoords='offset points')\nax.annotate(\"P\", proj_a_bc, xytext=(10, -10), textcoords='offset points')\nax.annotate(\"x\", (proj_a_bc + a) / 2, xytext=(10, 0), textcoords='offset points')\n\nax.annotate(r\"$b_1$\", (proj_a_bc + c) / 2, xytext=(10, 0), textcoords='offset points')\nax.annotate(r\"$b_2$\", (proj_a_bc + b) / 2, xytext=(10, 0), textcoords='offset points')\nax.annotate(r\"$\\theta$\", c, xytext=(4, 15), textcoords='offset points')\n\nax.plot([a[0], b[0]], [a[1], b[1]], 'r', label='a')\nax.plot([a[0], c[0]], [a[1], c[1]], 'b', label='b')\nax.plot([c[0], b[0]], [c[1], b[1]], 'g', label='c')\nax.plot([a[0], proj_a_bc[0]], [a[1], proj_a_bc[1]], \"--\", label='proj_a_bc')\n\nax.plot(proj_a_bc[0], proj_a_bc[1], 'o', label='proj_a_b')\n\nax.set_xlim(0.5, 5.5)\nax.set_ylim(0.5, 5.5)\nax.set_xlabel('x')\nax.set_ylabel('y')\n# ax.legend()\n\nText(0, 0.5, 'y')\n\n\n\n\n\n\n\n\n\n\nnp.dot(b - c, a - proj_a_bc)\n\n0.0\n\n\nDer Beweis des Kosinussatzes ist. Wir möchten die Länge der Seite c berechnen, indem wir den Winkel zwischen den Seiten a und b benutzen. Falls das Dreieck rechtwinklig ist (\\theta = 90^{\\circ} oder \\theta = \\pi / 2), dann können wir den Satz des Pythagoras benutzen:\n\nc^2 = a^2 + b^2\n\nFür beliebige Dreiecke, können wir das Dreieck in zwei rechtwinklige Dreiecke aufteilen, indem wir eine Höhe von einem Eckpunkt auf die gegenüberliegende Seite ziehen.\nNun haben wir die Dreiecke ACP und BAP, wobei P der Punkt ist, an dem die Höhe die Seite c schneidet (XXX, check).\nFür die Länge der Seite c gilt nach dem Satz des Pythagoras:\n\nc^2 = b_2^2 + x^2\n\nIn dieser Gleichung sind b_2 und x die Längen der Seiten des rechtwinkligen Dreiecks BAP, allerdings sind diese zunächst unbekannt. Wir können diese Längen jedoch in Beziehung zu den Seiten des ursprünglichen Dreiecks setzen.\nÜber b_1 und b_2 wissen wir, dass sich diese zu b summieren:\n\nb_1 + b_2 = b\n\nDie Längen der Seiten b_1 und b_2 können wir ausdrücken als\n\n\\begin{align*}\n\\cos(\\theta) & = \\frac{b_1}{a} \\\\\n\\sin(\\theta) & = \\frac{x}{a}\n\\end{align*}\n\nNun können wir mit diesen Ausdrücken einsetzen und erhalten\n\n\\begin{align*}\nc^2 & = b_2^2 + x^2 \\\\\n    & = (b - b_1)^2 + (a \\sin \\theta)^2 \\\\\n    & = b^2 - 2b_1b + b_1^2 + a^2 \\sin^2 \\theta \\\\\n    & = b^2 - 2 b (a \\cos\\theta) + (a \\cos\\theta)^2 + a^2\\sin^2\\theta \\\\\n    & = b^2 - 2ab\\cos\\theta + a^2(\\cos^2\\theta) + a^2\\sin^2\\theta \\\\\n    & = b^2 - 2ab\\cos\\theta + a^2(\\cos^2\\theta + \\sin^2\\theta) \\\\\n    & = b^2 - 2ab\\cos\\theta + a^2\n\\end{align*}\n\nAm Ende haben wir den Satz benutzt, dass \\cos^2\\theta + \\sin^2\\theta = 1.\nAm Ende können wir den Spezialfall für rechtwinklige Dreiecke wiederherstellen, indem wir \\theta = 90^{\\circ} setzen. Da \\cos(90^{\\circ}) = 0 und \\sin(90^{\\circ}) = 1 ist, erhalten wir einfach den Satz des Pythagoras zurück:\n\nc^2 = a^2 + b^2 - 2ab\\cos(90^{\\circ}) = a^2 + b^2\n\nDen Kosinussatz können können wir benutzen, um eine Eigenschaft des Skalarprodukts zu zeigen. Wir wissen, dass das Skalarprodukt zweier Vektoren \\vec{a} und \\vec{b} definiert ist als\n\n\\vec{a} \\cdot \\vec{b} = a^T b = \\sum_{i=1}^n a_i b_i\n\nund das die quadrierte Länge eines Vektors \\vec{c} definiert ist als\n\n\\|\\vec{c}\\|^2 = c^T c = \\sum_{i=1}^n c_i^2\n\nNun können wir den Kosinussatz benutzen, um die Beziehung zwischen dem Skalarprodukt, den Längen zweier Vektoren und dem Winkel zwischen den beiden Vektoren zu zeigen.\nDer Kosinussatz besagt, dass\n\n|c|^2 = |a|^2 + |b|^2 - 2|a||b|\\cos(\\theta)\n\nDer Vektor \\vec{c} ist aber auch gleich der Differenz zwischen den Vektoren \\vec{a} und \\vec{b}:\n\n\\vec{c} = \\vec{a} - \\vec{b}\n\nDaraus folgt, dass wir den Kosinussatz auch schreiben können als\n\n\\begin{align*}\n|a - b|^2 & = |a|^2 + |b|^2 - 2|a||b|\\cos(\\theta) \\\\\n(a - b)^T (a - b) & = |a|^2 + |b|^2 - 2|a||b|\\cos(\\theta) \\\\\na^T a - 2 a^T b + b^T b & = |a|^2 + |b|^2 - 2|a||b|\\cos(\\theta) \\\\\n|a|^2 - 2 a^Tb + |b|^2 & = |a|^2 + |b|^2 - 2|a||b|\\cos(\\theta) \\\\\n- 2 a^Tb & = - 2|a||b|\\cos(\\theta) \\\\\na^Tb & = |a||b|\\cos(\\theta)\n\\end{align*}\n\n\nx1 = np.array([1, 2])\n\nnp.sum(x1**2)\n\n5\n\n\n\nnp.linalg.norm(x1)\n\n2.23606797749979",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Elemente der Linearen Algebra</span>"
    ]
  },
  {
    "objectID": "Appendices/04-LA.html#die-cauchy-schwarz-ungleichung",
    "href": "Appendices/04-LA.html#die-cauchy-schwarz-ungleichung",
    "title": "Anhang C — Elemente der Linearen Algebra",
    "section": "C.5 Die Cauchy-Schwarz-Ungleichung",
    "text": "C.5 Die Cauchy-Schwarz-Ungleichung\nDie Cauchy-Schwarz-Ungleichung ist eine Relation zwischen den Komponenten zweier Vektoren. Die Cauchy-Schwarz-Ungleichung besagt, dass das Skalarprodukt zweier Vektoren kleiner oder gleich dem Produkt der Längen der beiden Vektoren ist:\n\nTheorem C.1 Es seien \\vec{a} und \\vec{b} zwei Vektoren mit derselben Dimension (ansonsten ist das Skalarprodukt nicht einmal definiert). Dann gilt:\n|\\vec{a} \\cdot \\vec{b}| \\leq \\|\\vec{a}\\| \\cdot \\|\\vec{b}\\|\nDie Cauchy-Schwarz-Ungleichung kann auch als\n-1 \\leq \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\| \\cdot \\|\\vec{b}\\|} \\leq 1\ngeschrieben werden.\n\n\n\n\n\n\n\nBeweis\n\n\n\n\n\nAus der Definition des Skalarprodukts folgt:\n\\vec{a} \\cdot \\vec{a} = \\|\\vec{a}\\|^2 \\geq 0\nDer Kosinussatz gibt uns die Beziehung zwischen dem Skalarprodukt und dem Winkel zwischen den beiden Vektoren:\n\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|^2 = \\|\\vec{a}\\| \\cdot \\|\\vec{b}\\| \\cdot \\cos(\\theta)\nDa der Kosinus des Winkels zwischen zwei Vektoren im Intervall [-1, 1] liegt, folgt:\n-1 \\leq \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\| \\cdot \\|\\vec{b}\\|} \\leq 1",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Elemente der Linearen Algebra</span>"
    ]
  }
]